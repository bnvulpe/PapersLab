{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f8d8f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "client = Elasticsearch('http://elasticsearch:9200')\n",
    "# Create the index if it does not exist\n",
    "index_name = 'all_papers_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cc3a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'arxivId': '1206.6927', 'authors': [{'name': 'Flynn, Cheryl J.'}, {'name': 'Perry, Patrick O.'}], 'contributors': ['Cheryl'], 'outputs': ['https://api.core.ac.uk/v3/outputs/423162609'], 'createdDate': '2012-07-05T14:02:54', 'dataProviders': [{'id': 144, 'name': '', 'url': 'https://api.core.ac.uk/v3/data-providers/144', 'logo': 'https://api.core.ac.uk/data-providers/144/logo'}, {'id': 4786, 'name': '', 'url': 'https://api.core.ac.uk/v3/data-providers/4786', 'logo': 'https://api.core.ac.uk/data-providers/4786/logo'}], 'abstract': 'Biclustering, the process of simultaneously clustering the rows and columns\\nof a data matrix, is a popular and effective tool for finding structure in a\\nhigh-dimensional dataset. Many biclustering procedures appear to work well in\\npractice, but most do not have associated consistency guarantees. To address\\nthis shortcoming, we propose a new biclustering procedure based on profile\\nlikelihood. The procedure applies to a broad range of data modalities,\\nincluding binary, count, and continuous observations. We prove that the\\nprocedure recovers the true row and column classes when the dimensions of the\\ndata matrix tend to infinity, even if the functional form of the data\\ndistribution is misspecified. The procedure requires computing a combinatorial\\nsearch, which can be expensive in practice. Rather than performing this search\\ndirectly, we propose a new heuristic optimization procedure based on the\\nKernighan-Lin heuristic, which has nice computational properties and performs\\nwell in simulations. We demonstrate our procedure with applications to\\ncongressional voting records, and microarray analysis.Comment: 40 pages, 11 figures; R package in development at\\n  https://github.com/patperry/biclustp', 'documentType': 'research', 'doi': '10.1214/19-ejs1667', 'downloadUrl': 'http://arxiv.org/abs/1206.6927', 'fieldOfStudy': None, 'fullText': 'Consistent Biclustering\\nCheryl J. Flynn and Patrick O. Perry\\nNew York University\\nNovember 17, 2015\\nAbstract\\nBiclustering, the process of simultaneously clustering the rows and columns of a\\ndata matrix, is a popular and effective tool for finding structure in a high-dimensional\\ndataset. Many biclustering procedures appear to work well in practice, but most do not\\nhave associated consistency guarantees. To address this shortcoming, we propose a new\\nbiclustering procedure based on profile likelihood. The procedure applies to a broad\\nrange of data modalities, including binary, count, and continuous observations. We\\nprove that the procedure recovers the true row and column classes when the dimensions\\nof the data matrix tend to infinity, even if the functional form of the data distribution\\nis misspecified. The procedure requires computing a combinatorial search, which can\\nbe expensive in practice. Rather than performing this search directly, we propose a\\nnew heuristic optimization procedure based on the Kernighan-Lin heuristic, which has\\nnice computational properties and performs well in simulations. We demonstrate our\\nprocedure with applications to congressional voting records, and microarray analysis.\\nKEY WORDS: Biclustering; Block Model; Profile Likelihood; Congressional Voting;\\nMicroarray Data.\\n1\\nar\\nX\\niv\\n:1\\n20\\n6.\\n69\\n27\\nv3\\n  [\\nsta\\nt.M\\nE]\\n  8\\n Ja\\nn 2\\n01\\n6\\n1 Introduction\\nSuppose we are given a data matrix X = [Xij], and our goal is to cluster the rows and\\ncolumns of X into meaningful groups. For example, Xij can indicate whether or not user i\\nhas an interest in product j, and our goal is to segment users and products into relevant\\nsubgroups. Alternatively, Xij could be the log activation level of gene j in patient i; our\\ngoal is to seek groups of patients with similar genetic profiles, while at the same time finding\\ngroups of genes with similar activation levels. The general simultaneous clustering problem is\\nknown by many names, including direct clustering (Hartigan, 1972), block modeling (Arabie\\net al., 1978), biclustering (Mirkin, 1996), and co-clustering (Dhillon, 2001).\\nEmpirical results from a broad range of disciplines indicate that biclustering is useful in\\npractice. Ungar and Foster (1998) and Hofmann (1999) found that biclustering helps identify\\nstructure in collaborative filtering contexts with heterogeneous users and sparsely-observed\\npreferences. Eisen et al. (1998) used microarray data to simultaneously cluster genes and\\nconditions, finding that genes with similar functions often cluster together. Harpaz et al. (2010)\\napplied biclustering methods to a Food and Drug Adminstration report database, identifying\\nassociations between certain active ingredients and adverse medical reactions. Several other\\napplications of biclustering exist (Cheng and Church, 2000; Getz et al., 2000; Lazzeroni and\\nOwen, 2002; Kluger et al., 2003); Madeira and Oliveira (2004) give a comprehensive survey.\\nPractitioners interested in biclustering have used a variety of different algorithms to\\nachieve their results. Clearly, many of these algorithms work well in practice, but they\\nare often ad-hoc, and there are no rigorous guarantees as to their performance. Without\\nthese guarantees practitioners cannot be assured that their discoveries from biclustering will\\ngeneralize or be reproducible; collecting more data may lead to completely different clusters.\\nThere are two approaches to evaluating the theoretical performance of these procedures.\\nThe first is to define a higher-level learning task, and evaluate procedures using a task-\\ndependent measure of generalization performance (Seldin and Tishby, 2009, 2010). We instead\\nconsider the alternative approach, which is to consider the problem purely as an unsupervised\\n2\\nlearning task. In this case, the procedure is evaluated based on the identified biclusters, where\\na reasonable goal is consistent biclustering.\\nOur first contribution is to formalize biclustering as an estimation problem. We do this by\\nintroducing a probabilistic model for the data matrix X, where up to permutations of the\\nrows and columns, the expectation of X has a block structure. Next, we make distributional\\nassumptions on the elements of X and derive a clustering objective function via profile-\\nlikelihood (Murphy and van der Vaart, 2000). Finally, we show that the maximum profile\\nlikelihood estimator performs well when the distributional assumptions are not satisfied, in\\nthe sense that it is still consistent. To our knowledge, this is the first general consistency\\nresult for a biclustering algorithm.\\nUnfortunately, it is computationally intractable to compute the maximum profile likelihood\\nestimator. It is for this reason that Ungar and Foster (1998), who used a similar probabilistic\\nmodel for the data matrix, dismissed likelihood-based approaches as computationally infeasible.\\nOur second contribution, then, is to propose a new approximation algorithm for finding a local\\nmaximizer of the biclustering profile likelihood. Our algorithm is based on the Kernigham-Lin\\nheuristic (Kernighan and Lin, 1970), which was employed by Newman (2006) for clustering\\nnetwork data. This is a greedy optimization procedure, so it is not guaranteed to find a\\nglobal optimum. To mitigate this, we run the fitting procedure repeatedly with many random\\ninitializations; as we increase the number of initializations, the probability that the algorithm\\nfinds the global optimum increases. We show that this procedure has low computational\\ncomplexity and it performs well in practice.\\nOur work was inspired by recent developments in clustering methods for symmetric binary\\nnetworks. In that context, X is an n-by-n symmetric binary matrix, and the clusters for the\\nrows of X are the same as the clusters for the columns of X. Bickel and Chen (2009) used\\nmethods similar to those used for proving consistency of M-estimators to derive results for\\nnetwork clustering when n tends to infinity. This work was later extended by Choi et al. (2012),\\nwho allow the number of clusters to increase with n; Zhao et al. (2011), who allow for nodes\\n3\\nnot belonging to any cluster; and Zhao et al. (2012), who incorporate individual-specific effects.\\nIn parallel to this work, Rohe et al. (2011) study the performance of spectral clustering for\\nsymmetric binary networks; and Rohe and Yu (2012) study spectral methods for unsymmetric\\nbinary networks.\\nIn our report we have extended methods originally developed for an extremely specialized\\ncontext (symmetric binary networks) to handle clustering for arbitrary data matrices. Using\\nstandard conditions, we have been able to generalize the Bickel and Chen (2009) results\\nbeyond Bernoulli random variables. To our knowledge, this is the first time methodologies\\nfor binary networks have been used to study general biclustering methods. Notably, our\\nextensions can handle a variety of data distributions, and they can handle both dense and\\nsparse data matrices.\\nThe main text of the paper is organized as follows. First Section 2 describes the theoretical\\nsetup and Section 3 presents our main result with a heuristic proof. Then, Section 4 describes\\nthe formal theoretical framework and states the rigorous consistency results. Next, Section 5\\npresents our approximation algorithm. Using this algorithm, Section 6 corroborates the\\ntheoretical findings through a simulation study, and Section 7 presents applications to a\\nmicroarray and a congressional voting dataset. Section 8 presents some concluding remarks.\\nThe supplementary appendices include additional proofs, empirical results, and an application\\nto a movie review dataset.\\n2 Estimation problem and criterion functions\\nOur first task is to formalize biclustering as an estimation problem. To this end, let X =\\n[Xij ] ∈ Rm×n be a data matrix. We follow the network clustering literature and posit existence\\nof K row classes and L column classes, such that the mean value of entry Xij is determined\\nsolely by the classes of row i and column j. That is, there is an unknown row class membership\\nvector c ∈ Km, an unknown column class membership vector d ∈ Ln, and an unknown\\n4\\nmean matrix M = [µkl] ∈ RK×L such that\\nEXij = µcidj . (2.1)\\nWe refer to model (2.1) as a block model, after the related model for undirected networks\\nproposed by Holland et al. (1983). When a block model is in force, biclustering the rows and\\ncolumns of the data matrix is equivalent to estimating c and d.\\nNot all block models give rise to well-defined estimation problems. To ensure that K\\nand L are well-defined, we require that each class has at least one member, and that no two\\nclasses have the same mean vector. Formally, define row class proportion vector p ∈ RK\\nwith element pa = m\\n−1∑\\ni I(ci = a) equal the proportion of nodes with row class a. Also,\\ndefine column class proportion vector q ∈ RL with element qb = n−1\\n∑\\nj I(dj = b) equal to\\nthe proportion of nodes with column class b. We require that every element of p and q be\\nnonzero. To ensure that the mean vectors of the row classes are distinct, we require that no\\ntwo rows of M are identical. Similarly, we require that no two columns of M are identical.\\nWe estimate the clusters by assigning labels to the rows and columns of X, codified in\\nvectors g ∈ Km and h ∈ Ln. Ideally, g and h match c and d. Note we are assuming that the\\ntrue numbers of row and column clusters, K and L, are known, or they have been correctly\\nestimated by some model selection procedure. We measure the performance of a particular\\nlabel assignment through the corresponding confusion matrix. Specifically, for row and column\\nlabel assignments g and h, define normalized confusion matrices C ∈ RK×K and D ∈ RL×L\\nby\\nCak =\\n1\\nm\\n∑\\ni\\nI(ci = a, gi = k), Dbl =\\n1\\nn\\n∑\\nj\\nI(dj = b, hj = l).\\nEntry Cak is the proportion of nodes with class a and label k; entry Dbl is defined similarly.\\nThese matrices are normalized so that C1 = p and D1 = q are the class proportion vectors,\\nand CT1 = pˆ and DT1 = qˆ are the label proportion vectors. If C and D are diagonal, then\\nthe assigned labels match the true classes. More generally, if C and D can be made diagonal\\n5\\nby permuting their columns, then the partition induced by the labels matches the partition\\ninduced by the classes. The goal, then, is to find row and column labellings such that C and\\nD are permutations of diagonal matrices.\\nIn practice, we cannot estimate C and D directly, because we do not have knowledge of\\nthe true row and column classes. To evaluate the quality of a biclustering, we need a surrogate\\ncriterion function. Analogously to Bickel and Chen (2009), we employ profile-likelihood for\\nthis purpose.\\nIn Bickel and Chen’s setting, the data are binary, so there is a natural data likelihood\\nwhich arises from the Bernoulli distribution. Our setting is more general, with X ij allowed\\nto be a count or a continuous measurement, so there are many possible choices for the\\nelement densities. We proceed by initially assuming that the elements of X are sampled\\nfrom distributions in a single-parameter exponential family. Conditional on c and d, the\\nelements of X are independent, and entry Xij has density g(x; ηcidj) with respect to some\\nbase measure ν, where\\ng(x; η) = exp{xη − ψ(η)};\\nψ(η) is the cumulant generating function, and ηkl = (ψ\\n′)−1(µkl) is the natural parameter.\\nLater, we will relax the assumption of the specific distributional form.\\nWith labels g and h, the complete data log-likelihood is\\nl(g,h,M ) =\\n∑\\nk,l\\n∑\\ni,j\\n{Xijηkl − ψ(ηkl)} I(gi = k, hj = l)\\n= mn\\n∑\\nk,l\\npˆk qˆl {X¯kl ηkl − ψ(ηkl)},\\nwhere pˆk = m\\n−1∑\\ni I(gi = k) and qˆl = n\\n−1∑\\nj I(hj = l) are the estimated class proportions\\nand X¯kl = {\\n∑\\ni,j I(gi = k, hj = l)}−1\\n∑\\ni,j Xij I(gi = k, hj = l) is the estimated cluster mean.\\nWe get the profile log-likelihood by maximizing the log-likelihood over the mean parameter\\n6\\nmatrix M :\\npl(g,h) = sup\\nM\\nl(g,h,M ) = mn\\n∑\\nk,l\\npˆk qˆl ψ\\n∗(X¯kl),\\nwhere ψ∗(x) = supη{xη − ψ(η)} is the convex conjugate of ψ. We refer to ψ∗ as the relative\\nentropy function since ψ∗(µ) is equal to the Kullback-Leiber divergence of the base measure ν\\nfrom the distribution in the exponential family with mean µ (Brown, 1986).\\nFollowing the above derivation, a natural criterion for the quality of labeling (g,h) is\\nthe profile log-likelihood pl(g,h). In the sequel, we consider a far more general setting. We\\nconsider criterion functions of the form\\nF (g,h) =\\n∑\\nk,l\\npˆk qˆl f(X¯kl), (2.2)\\nwhere f is any smooth convex function. Following the derivation above, we refer to F as a\\nprofile likelihood and we refer to f as the corresponding relative entropy function. However,\\nwe do not assume that likelihood has been correctly specified. In particular, as long as the\\nblock model (2.1) is in force, the elements of X can have arbitrary distributional forms, not\\nnecessarily belonging to any exponential family. We explicitly allow for heteroscedasticity and\\ndistributional misspecification. We show that under mild technical conditions, the maximizer\\nof F is a consistent estimator of the true row and column classes.\\n3 Heuristic justification\\nIn Section 2, we defined a formal biclustering estimation problem and we motivated a class of\\ncriterion functions for this problem based on profile likelihood. In this section, we investigate\\nthe behavior of the criterion functions. In particular, we outline a heuristic argument which\\nshows that the row and column labels found by maximizing these criterion functions are good\\nestimates of the true row and column classes. Formal statements of the results and their\\nproofs are given in Section 4 and Supplement A.\\n7\\nAs noted in Section 1, the main thrust of our theoretical results are similar to that used in\\nthe literature on clustering for symmetric binary networks initiated by Bickel and Chen (2009)\\nand extended by Choi et al. (2012), Zhao et al. (2011) and Zhao et al. (2012). The main\\npoint of departure from this previous work are that we work with arbitrary data modalities\\ninstead of symmetric binary matrices.\\nLet X ∈ Rm×n be a data matrix drawn from an identifiable block model (2.1) with row\\nand column classes c ∈ Km and d ∈ Ln and mean matrix M ∈ RK×L. Let p, and q be as\\ndefined in Section 2. For any row and column labeling g and h, let pˆ, qˆ, and X¯ be the\\ncorresponding estimates of p, q, and M , and let C and D be the confusion matrices. Let\\nF be a profile likelihood criterion function as in (2.2) with corresponding relative entropy\\nfunction f , assumed to be smooth and strictly convex.\\nWe now outline a series of results which show that the maximizers of F are good estimates\\nof the true row and column classes.\\nProposition 3.1. The criterion function F is uniformly close to a “population criterion\\nfunction” G which only depends on the confusion matrices.\\nIf n and m are large, then for any choice of g and h, the estimated cluster mean X¯kl will\\nbe close to Ekl, the average value of EXij over the block defined by labels k and l. This\\nquantity can be computed in terms of the confusion matrices as\\nEkl =\\n∑\\ni,j\\n∑\\na,b µab I(ci = a, gi = k) I(dj = b, hj = l)∑\\ni,j I(gi = k, hj = l)\\n=\\n[CT MD]kl\\n[CT1]k[D\\nT1]l\\n.\\nBy applying Bernstein’s inequality, one can show that Ekl is close to X¯kl uniformly over all\\nchoices of g and h. Thus, we get the population criterion function by replacing X¯kl with Ekl.\\nFor each non-negative vector t ∈ RN+ define Ct to be the set of N ×N normalized confusion\\nmatrices with fixed row sums: Ct = {W ∈ RN×N+ : W1 = t}. The population version of F is\\n8\\na function of the row and column confusion matrices, G : Cp × Cq → R, with\\nG(C,D) =\\n∑\\nk,l\\n[CT1]k [D\\nT1]l f\\n( [CTMD]kl\\n[CT1]k [D\\nT1]l\\n)\\n.\\nSince X¯kl is uniformly close to Ekl, under mild regularity conditions on f , the criterion F (g,h)\\nis uniformly close to G(C,D). Proposition ?? (Supplement A) contains a rigorous statement\\nof this result.\\nProposition 3.2. The population criterion function G is self-consistent.\\nSelf-consistency is an important property for any criterion function, which implies that\\nin the absence of noise, the criterion function will be maximized at the truth (Tarpey and\\nFlury, 1996). In our context, self-consistency means that G is maximized when C and D are\\npermutations of diagonal matrices.\\nThe self-consistency of G follows from the strict convexity of f :\\nG(C,D) =\\n∑\\nk,l\\n[CT1]k [D\\nT1]l f\\n( [CTMD]kl\\n[CT1]k [D\\nT1]l\\n)\\n≤\\n∑\\nk,l\\n∑\\na,b\\nCakDblf(µab)\\n=\\n∑\\na,b\\npa qb f(µab).\\nIf M has no two identical rows and no two identical columns, then exact equality holds only\\nwhen C and D are permutations of diagonal matrices. Thus, G is maximized when the row\\nand column class partitions match the label partitions. Proposition ?? (Supplement A) gives\\na refined self-consistency result with a more complete characterization of the behavior of G\\nnear its maxima.\\nProposition 3.3. Under enough regularity, the maximizer of the criterion function F is\\nclose to the true row and column class partition.\\n9\\nThis is a direct consequence of Propositions 3.1 and 3.2. The criterion F is uniformly\\nclose to the population criterion G, and G is maximized at the true class partitions. Thus,\\nthe maximizer of F is close to the maximizer of G. Importantly, Proposition 3.3 does\\nnot require any distributional assumptions on the data matrix X beyond its expectation\\nsatisfying the block model. In particular this result can be applied to binary matrices, count\\ndata, and continuous data. Theorems 4.1 and 4.2 contain precise statements analogous to\\nProposition 3.3.\\n4 Rigorous Theoretical Results\\nHere we provide formal statements of the main result from Section 3. The proofs of these\\nresults are contained in Supplement A\\nWe work in an asymptotic framework, where the dimensions of the data matrix tend to\\ninfinity. Let Xn ∈ Rm×n be a sequence of data matrices indexed by n, with m = m(n) and\\nm(n)→∞ as n→∞. We will also suppose that n/m→ γ for some finite constant γ > 0;\\nthis assumption is not essential, but it simplifies the assumption and result statements.\\nSuppose that for each n there exists a row class membership vector cn ∈ Km and a column\\nclass membership vector dn ∈ Ln. We assume that there exist vectors p ∈ RK and q ∈ RL\\nsuch that pˆk(c)→ pk and qˆl(d)→ ql as n→∞ almost surely for all k and l; this assumption\\nis satisfied, for example, if the class labels are independently drawn from a multinomial\\ndistribution. When there is no ambiguity, we omit the subscript n.\\nWe define the mean matrix M = [µkl] ∈ RK×L as in Section 3, but allow it to possibly vary\\nwith n. To model sparsity in X, we allow M to tend to 0. To avoid degeneracy, we suppose\\nthat there exists a sequence ρ and a fixed matrix M 0 ∈ RK×L such that ρ−1 M → M 0.\\nDenote by M0 ∈ R the convex hull of the entries of M 0. Let M be a neighborhood of M0.\\n10\\nTo adjust for the sparsity, we redefine the criterion and population criterion functions as\\nF (g,h) =\\n∑\\nk,l\\npˆkqˆlf(ρ\\n−1X¯kl),\\nG(C,D) =\\n∑\\nk,l\\n[CT1]k[D\\nT1]lf\\n( [CTM 0D]kl\\n[CT1]k [D\\nT1]l\\n)\\n.\\nWe discuss these modifications and the role of ρ in Section ??.\\nWe only consider nontrivial partitions; to this end, for ε > 0, define Jε, the set of nontrivial\\nlabellings as\\nJε = {g,h : pˆk(g) > ε, qˆl(h) > ε}.\\n4.1 Assumptions\\nWe require the following regularity conditions:\\n(C1) The biclusters are identifiable: no two rows of the M 0 are equal, and no two columns\\nof M 0 are equal.\\n(C2) The relative entropy function is locally strictly convex: f ′′(µ) > 0 when µ ∈M.\\n(C3) The third derivative of the relative entropy function is locally bounded: |f ′′′(µ)| is\\nbounded when µ ∈M.\\n(C4) The average variance of the elements is of the same order as ρ:\\nlim sup\\nn→∞\\n1\\nρmn\\n∑\\ni,j\\nE[(Xij − µcidj)2] <∞.\\n(C5) The mean matrix does not converge to zero too quickly:\\nlim sup\\nn→∞\\nρ\\n√\\nnm =∞.\\n11\\n(C6) The elements satisfy a Lindeberg condition: for all ε > 0,\\nlim\\nn→∞\\n1\\nρ2mn\\n∑\\ni,j\\nE[(Xij − µcidj)2 I(|Xij − µcidj | > ερ\\n√\\nmn)] = 0.\\nCondition (C1) is necessary for the biclusters to be identifiable, while (C2) and (C3) are mild\\nregularity conditions on the entropy function.\\nCondition (C4) is trivially satisfied for dense data and is satisfied for Binomial and Poisson\\ndata so long as ρ−1M → M 0. However, this condition cannot handle arbitrary sparsity.\\nFor example, if the elements of X are distributed as Negative Binomial random variables,\\nthen condition (C4) requires that the product of the mean and the dispersion parameter\\ndoes not tend to infinity. In other words, for sparse count data, the counts cannot be too\\nheterogeneous.\\nCondition (C5) places a sparsity restriction on the mean matrix. Zhao et al. (2012)\\nused the same assumption to establish weak consistency for network clustering. A variant\\nLyaponuv’s condition (Varadhan, 2001) implies (C6). That is, if\\nlim\\nn→∞\\n1\\n(ρ\\n√\\nmn)2+δ\\n∑\\ni,j\\nE |Xij − µcidj |2+δ = 0\\nfor some δ > 0, then (C6) follows. In particular, for dense data (ρ bounded away from zero),\\nuniformly bounded (2 + δ) absolute central moments for any δ > 0 is sufficient. For many\\ntypes of sparse data, including Bernoulli or Poisson data with ρ converging to zero, (C5) is a\\nsufficient condition for (C6).\\nTheorem 4.1. Fix any ε > 0 with ε < mina{pa} and ε < minb{qb}. Let (gˆ, hˆ) satisfy\\nF (gˆ, hˆ) = maxJε F (g,h). If conditions (C1)–(C6) hold, then all limit points of C(gˆ) and\\nD(hˆ) are permutations of diagonal matrices, i.e. the proportions of mislabeled rows and\\ncolumns converge to zero in probability.\\n12\\nOur focus is on the cluster assignments, but, using the methods involved to prove Theo-\\nrem 4.1, it is possible to show that when the assumptions of Theorem 4.1 are in force, then\\nthe scaled estimate of the mean, ρ−1X¯kl converges in probability to the population quantity.\\n(This follows from Theorem 4.1 and Lemma ?? from Supplement A.)\\nUnder stronger distributional assumptions, we can use the methods of the proof to establish\\nfinite-sample results. For example, if we assume that the elements of X are Gaussian, then\\nthe following result holds.\\nTheorem 4.2. Fix any ε > 0. Let (gˆ, hˆ) satisfy F (gˆ, hˆ) = maxJε F (g,h). If the elements of\\nX are independent Gaussian random variables with constant variance σ2 and conditions (C1)–\\n(C3) hold, then for any 0 < δ < min\\n{\\n1, 8cσmax{K\\n2,L2}\\nτε2\\n}\\n,\\nPr\\n((\\nC(gˆ),D(hˆ)\\n)\\n/∈ Pδ ∩Qδ\\n)\\n≤ 2Km+1Ln+1 exp\\n{\\n− Tnτ\\n2ε4δ2\\n256c2σ2 max{K4, L4}\\n}\\n,\\nwhere c = sup |f ′(µ)| for µ in M,\\nTn = inf\\nk,l\\n{∑\\ni\\n∑\\nj\\nI(gi = k, hj = l)|(g,h) ∈ Jε\\n}\\n.\\nThe proof of this finite-sample result follows the same outline as the asymptotic result;\\nAppendix ?? (Supplement A) gives details.\\n5 Approximation algorithm\\nProposition 3.3 shows that the maximizer of the profile log-likelihood F will give a good\\nestimate of the true clusters. Unfortunately, finding this maximizer is computationally\\nintractable. Maximizing F is a combinatorial optimization problem with an exponentially-\\nlarge state space. To get around this, we will settle for finding a local optimum rather than a\\nglobal one. We present an algorithm for finding a local optimum that, in practice, has good\\nestimation performance.\\n13\\nOur approach is based on the Kernighan-Lin heuristic (Kernighan and Lin, 1970), which\\nNewman (2006) used for a related problem, network community detection. After inputting\\ninitial partitions for the rows and columns, we iteratively update the cluster assignments in a\\ngreedy manner. The algorithm works as follows:\\n1. Initialize the row and column labels g and h arbitrarily, and compute F .\\n2. Repeat until convergence:\\n(a) For each row i, determine which of the K possible label assignments for this row\\nis optimal, keeping all other row and column labels fixed. Do not perform this\\nassignment, but record the optimal label and the local improvement to F that\\nwould result if this assignment were to be made.\\n(b) For each column j, determine which of the L possible label assignments for this\\ncolumn is optimal, keeping all other row and column labels fixed. As in step 2a, do\\nnot perform this assignment, but record the optimal label and the local improvement\\nto F that would result if this assignment were to be made.\\n(c) In order of the local improvements recorded in steps 2a and 2b, sequentially perform\\nthe individual cluster reassignments determined in these steps, and record the\\nprofile likelihood after each reassignment. Note that these assignments are no\\nlonger locally optimal since the labels of many of the rows and columns change\\nduring this step. Thus, the profile likelihood could increase or decrease as we move\\nsequentially through the assignments.\\n(d) Out of the sequence of cluster assignments considered in step 2c, choose the one\\nthat has the highest profile likelihood.\\nSince the criterion function increases at each complete iteration, the algorithm will converge\\nto a local optimum.\\n14\\nThere is no guarantee that the local optimum found by the algorithm will be the global\\noptimum of the objective function F . To mitigate this deficiency, we will run the algorithm\\nrepeatedly with many different random initializations for g and h. Each initialization can\\ngive rise to a different local optimum. We choose the cluster assignment with the highest\\nvalue of F among all local optima found by the procedure. As we increase the number of\\nrandom initializations, the probability that the global optimum will be in this set will increase.\\nWe found that 100–250 initializations seem to suffice in the simulations and data examples.\\nThe main computational bottleneck is updating the value of F (g,h) as we update the\\nlabels g and h. We can do this efficiently by storing and incrementally updating the cluster\\nproportions pˆ and qˆ, the within-cluster row and column sums\\nRil =\\nn∑\\nj=1\\nXij I(hj = l) and Ckj =\\nm∑\\ni=1\\nXij I(gi = k),\\nand the block sums Bkl =\\n∑m\\ni=1Ril I(gi = k). Given the values of these quantities, we can\\ncompute the criterion F (g,h) with O(KL) operations.\\nIf we reassign the label of row i from k to k′, then it is straightforward to update pˆ with\\nO(1) operations. The values of the within-cluster row sums Ril remain unchanged. The new\\nvalues of the block sums are B′kl = Bkl −Ril and B′k′l = Bk′l +Ril for l = 1, . . . , L; the other\\nblock sums are unchanged. The expensive part of the update is recomputing the within-cluster\\ncolumn sums Ckj for row labels k and k\\n′ and each column j. These computations require\\nO(mn) operations if X is dense, and O(N) operations if X is sparse with at most N nonzero\\nelements and N ≥ max{m,n}.\\nOverall we must perform O(N+KL) operations to reassign the label of row i. Reassigning\\nthe label of column j has the same computational cost. Thus, one loop iteration in step 2\\nrequires O((m + n)(N + KL)) operations. For dense data, one iteration requires O((m +\\nn)(mn + KL)\\n)\\noperations. We do not have an upper bound on the number of iterations\\nuntil the algorithm converges, but in our experiments we found that empirically, 25 to 30\\n15\\niterations suffice. These iteration counts may seem small, but in fact each iteration performs\\nm row label assignments and n column label assignments. The convergence here is not a\\nresult of early stopping—we found that after 25–30 iterations, no possible local improvement\\nwas possible.\\nFor comparison, a spectral-based biclustering algorithm requires the top singular vectors of\\nthe data matrix, which can be gotten in roughly O(mn) operations using Lanczos or another\\nindirect method (Golub and Loan, 1996).\\n6 Empirical evaluation\\nHere we evaluate the performance of the profile likelihood based biclustering algorithm from\\nSection 5. We simulate data from a variety of regimes, including sparse binary data and\\ndense heavy-tailed continuous measurements. In these settings, we employ the following three\\nrelative entropy functions:\\nfBernoulli(µ) = µ log µ+ (1− µ) log(1− µ), (6.1a)\\nfPoisson(µ) = µ log µ− µ, (6.1b)\\nfGaussian(µ) = µ\\n2/2. (6.1c)\\nWe evaluate performance both when the profile likelihood is correctly specified and when the\\nrelative entropy function does not match the data distribution.\\nIn our simulations, we report the proportion of misclassified rows and columns by the\\nprofile likelihood based method (PL). We initialize partitions randomly, and then run the\\nimprovement algorithm from Section 5 until convergence. We use multiple random starting\\nvalues to minimize the possibility of finding a non-optimal stationary point.\\nWe compare our method to two other biclustering algorithms. The first algorithm is a\\nspectral biclustering algorithm, DI-SIM, motivated by a block model similar to ours (Rohe\\nand Yu, 2012). The algorithm finds the singular value decomposition of the data matrix X,\\n16\\nand then applies k-means clustering to the top left and right singular vector loadings. The\\nsecond algorithm we compare against, KM, ignores the interactions between the clusters, and\\napplies k-means separately to the rows and columns of X.\\nIn our first simulation, we generate sparse Poisson count data from a block model with\\nK = 2 row clusters and L = 3 column clusters. We vary the number of columns, n, between\\n200 to 1400 and we take the number of rows to be m = γn for γ ∈ {0.5, 1, 2}. To assign the\\ntrue row and column classes c and d, we sample independent multinomials with probabilities\\np = (0.3, 0.7) and q = (0.2, 0.3, 0.5). We choose the matrix of block parameters to be\\nM = [µab] =\\nb√\\nn\\n\\uf8eb\\uf8ec\\uf8ed0.92 0.77 1.66\\n0.17 1.41 1.45\\n\\uf8f6\\uf8f7\\uf8f8 ,\\nwhere b is chosen between 5 and 20; the entries of the matrix were chosen randomly, uniformly\\non the interval [0, 2]. We chose the 1/\\n√\\nn scaling so that the data matrix would be sparse,\\nwith O(√n) elements in each row. We generate the data conditional on the row and column\\nclasses as Xij | c,d ∼ Poisson(µcidj). We run all three methods with 250 random starting\\nvalues.\\nFigure 1 presents the average bicluster misclassification rates for each sample size and\\nTable 1 reports the standard deviations. In all of the scenarios considered, biclustering based\\non the profile likelihood criterion performs at least as well as the other methods, even when\\nthe relative entropy function is misspecified (using PL-Gaus instead of PL-Pois). Moreover\\nby looking at the standard deviations, we see that for the PL methods, the misclassification\\nrate seems to be converging to zero as we increase n.\\n17\\nl\\nl\\nl l l l l\\n200 400 600 800 1200\\n0.\\n0\\n0.\\n2\\n0.\\n4\\nm = 0.5n, b = 5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1200\\n0.\\n0\\n0.\\n2\\n0.\\n4\\nm = 0.5n, b = 10\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1200\\n0.\\n0\\n0.\\n2\\n0.\\n4\\nm = 0.5n, b = 20\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl l l l l l\\n200 400 600 800 1200\\n0.\\n00\\n0.\\n10\\n0.\\n20\\nm = 1n, b = 5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1200\\n0.\\n00\\n0.\\n10\\n0.\\n20\\nm = 1n, b = 10\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1200\\n0.\\n00\\n0.\\n10\\n0.\\n20\\nm = 1n, b = 20\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl l l l l l\\n200 400 600 800 1200\\n0.\\n00\\n0.\\n02\\n0.\\n04\\nm = 2n, b = 5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1200\\n0.\\n00\\n0.\\n02\\n0.\\n04\\nm = 2n, b = 10\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1200\\n0.\\n00\\n0.\\n02\\n0.\\n04\\nm = 2n, b = 20\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl PL KM DI−SIM PL−Gaus\\nFigure 1: Average misclassification rates for Poisson example over 100 simulations.\\nm = 0.5n\\nPL-Pois PL-Norm KM DS\\n200 0.0356 0.0147 0.0047 0.0878 0.0239 0.0070 0.0957 0.0438 0.0097 0.0403 0.0286 0.0114\\n400 0.0182 0.0064 0.0013 0.0313 0.0091 0.0025 0.0478 0.0145 0.0033 0.0279 0.0144 0.0043\\n600 0.0103 0.0034 0.0004 0.0153 0.0045 0.0011 0.0253 0.0068 0.0013 0.0186 0.0083 0.0026\\n800 0.0076 0.0024 0.0003 0.0104 0.0039 0.0006 0.0188 0.0050 0.0009 0.0140 0.0069 0.0013\\n1000 0.0054 0.0018 0.0003 0.0076 0.0029 0.0003 0.0122 0.0033 0.0004 0.0111 0.0040 0.0008\\n1200 0.0041 0.0010 0.0001 0.0061 0.0017 0.0003 0.0092 0.0021 0.0003 0.0092 0.0034 0.0007\\n1400 0.0036 0.0009 0.0001 0.0047 0.0014 0.0001 0.0077 0.0016 0.0001 0.0071 0.0023 0.0005\\nm = n\\nn b=5 b=10 b=20 b=5 b=10 b=20 b=5 b=10 b=20 b=5 b=10 b=20\\n200 0.0160 0.0039 0.0009 0.0415 0.0069 0.0010 0.0663 0.0119 0.0019 0.0339 0.0128 0.0030\\n400 0.0071 0.0017 0.0000 0.0101 0.0021 0.0004 0.0183 0.0034 0.0004 0.0152 0.0049 0.0009\\n600 0.0036 0.0005 0.0000 0.0051 0.0008 0.0000 0.0100 0.0014 0.0000 0.0088 0.0024 0.0002\\n800 0.0021 0.0003 0.0000 0.0035 0.0005 0.0000 0.0056 0.0009 0.0000 0.0064 0.0014 0.0001\\n1000 0.0017 0.0002 0.0000 0.0026 0.0004 0.0000 0.0043 0.0006 0.0000 0.0050 0.0009 0.0000\\n1200 0.0012 0.0001 0.0000 0.0020 0.0002 0.0000 0.0030 0.0004 0.0000 0.0036 0.0008 0.0000\\n1400 0.0007 0.0001 0.0000 0.0012 0.0001 0.0000 0.0018 0.0002 0.0000 0.0025 0.0005 0.0000\\nm = 2n\\nn b=5 b=10 b=20 b=5 b=10 b=20 b=5 b=10 b=20 b=5 b=10 b=20\\n200 0.0059 0.0010 0.0000 0.0118 0.0018 0.0000 0.0204 0.0027 0.0000 0.0141 0.0039 0.0000\\n400 0.0022 0.0001 0.0000 0.0033 0.0003 0.0000 0.0071 0.0005 0.0000 0.0056 0.0010 0.0000\\n600 0.0010 0.0000 0.0000 0.0014 0.0001 0.0000 0.0024 0.0002 0.0000 0.0025 0.0004 0.0000\\n800 0.0004 0.0000 0.0000 0.0008 0.0000 0.0000 0.0015 0.0000 0.0000 0.0017 0.0001 0.0000\\n1000 0.0002 0.0000 0.0000 0.0005 0.0000 0.0000 0.0009 0.0000 0.0000 0.0013 0.0000 0.0000\\n1200 0.0002 0.0000 0.0000 0.0003 0.0001 0.0000 0.0006 0.0001 0.0000 0.0009 0.0001 0.0000\\n1400 0.0000 0.0000 0.0000 0.0002 0.0000 0.0000 0.0005 0.0000 0.0000 0.0006 0.0000 0.0000\\nTable 1: Standard deviations for Poisson example over 100 simulations\\n18\\nSupplement B describes in detail the simulations for Bernoulli, Gaussian, and heavy-tailed\\nt data. These results are similar to the Poisson case. Our method performs at least as well as\\nthe other procedures in all cases and shows signs of convergence.\\nOverall, the simulations confirm the conclusions of Proposition 3.3, and they show that\\nour approximate maximization algorithm performs well. These results give us confidence that\\nprofile likelihood based biclustering can be used in practice.\\n7 Applications\\nIn this section we use profile-likelihood-based biclustering to reveal structure in two high-\\ndimensional datasets. For each example, we maximize the profile log-likelihood using the\\nalgorithm described in Section 5. An additional application example is provided in Supplement\\nB.\\n7.1 GovTrack\\nIn our first application of the proposed method, we cluster legislators and motions based on\\nthe roll-call votes from the 113th United States House of Representatives (years 2013–2014).\\nWe validate our method by showing that the clusters found by the method agree with the\\npolitical parties of the legislators.\\nAfter downloading the roll-call votes from govtrack.org, we form a data matrix X with\\nrows corresponding to the 444 legislators who voted in the House of Representatives, and\\ncolumns corresponding to the 545 motions voted upon. Even though there are only 435 seats\\nin the House of Representatives, 9 legislators were replaced mid-session when they resigned or\\n19\\ndied. We code the non-missing votes as\\nXij =\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n1 if legislator i voted “Yea” on motion j,\\n0 if legislator i voted “Nay” on motion j,\\nNA if legislator i did not vote on motion j.\\nNot all legislators vote on all motions, and 7% of the data matrix entries are missing, coded\\nas NA. If we assume that the missing data mechanism is ignorable, then it is straightforward\\nto extend our fitting procedure to handle incomplete data matrices. Specifically, to handle the\\nmissing data, we replace sums over all matrix entries with sums over all non-missing matrix\\nentries.\\nTo choose the number of row clusters, K, and the number of column clusters, L, we fit\\nall 100 candidate models with K and L each ranging between 1 and 10. Figure 2 plot show\\nthe deviance (twice the negative log-likelihood) plotted as a function of K and L. The left\\n“scree” plot shows that for most values of K, increasing L from 1 to 4 has a large effect of the\\ndeviance, but increases L from 4 to a larger value has only a minor effect. Similarly, the right\\nplot shows that increasing K from 1 to 2 causes a large drop in the deviance, but increasing\\nK to a larger value has only a minor effect. Together, these plot suggest that we should pick\\nK = 2 and L = 4.\\nTo guard against the possibility of the bicluster algorithm finding a local rather than\\nglobal optimum, we used 100 random initializations, and chose the row and column cluster\\nassignments with the highest log-likelihood. To check the robustness of this assignment, we\\nincreased the number of random initializations up to 1000. Even with 10 times as many\\nrestarts, we still found the same optimal log-likelihood.\\nFigure 3 shows a heatmap constructed after biclustering the data into 2 row clusters\\nand 4 column clusters. The two row-clusters found by the algorithm completely recover the\\npolitical parties of the legislators (every legislator in row cluster 1 is a Democrat, and every\\n20\\nll l l l l l l l l\\nl\\nl l l l l l l l l\\n2 4 6 8 10\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nK\\nD\\nev\\nia\\nnc\\ne\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nL\\n2 4 6 8 10\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nFigure 2: GovTrack likelihood for different values of K and L\\nlegislator in row cluster 2 is a Republican). The fact that we were able to recover the political\\nparty provides us some confidence that the algorithm can find meaningful clusters in practice.\\nThe column clusters reveal four types of motions: (1) motions with strong support from\\nboth parties; (2) motions with moderate support from both parties; (3) motions with strong\\nDemocratic support; (4) motions with strong Republican support.\\nWe compared the clusters found by our method with those found by the two competing\\nmethods, DI-SIM and KM. The competing methods do not directly handle missing data, so\\nfor these methods we code “Yea” as +1, “Nay“ as −1, and “Missing” as 0. DI-SIM returns the\\nsame row-clusters, but classified 50 columns (9%) differently. KM placed 10 Republicans into\\nthe majority-Democrat cluster; it classified 45 motions (8%) differently from profile likelihood.\\nThe fact that all three methods are giving similar answers gives us confidence that the column\\nclusters are meaningful.\\n21\\nFigure 3: Heatmap generated from GovTrack data reflecting the voting patterns in the in the\\ndifferent biclusters. Blue (dark) identifies “Yea” votes, yellow (light) identifies “Nay” votes,\\nand white identifies missing votes.\\n7.2 AGEMAP\\nBiclustering is commonly used for microarray data to visualize the activation patterns of\\nthousands of genes simultaneously. It is used to identify patterns and discover distinguishing\\nproperties between genes and individuals. We use the AGEMAP dataset (Zahn et al., 2007)\\nto illustrate this process.\\nAGEMAP is a large microarray data set containing the log expression levels for 40 mice\\nacross 8,932 genes measured on 16 different tissue types. For this analysis, we restrict attention\\nto two tissue types: cerebellum and cerebrum. The 40 mice in the dataset belong to four age\\ngroups, with five males and five females in each group. One of the mice is missing data for\\nthe cerebrum tissue so it has been removed from the dataset.\\nOur goal is to uncover structure in the gene expression matrix. We bicluster the 39\\n× 17,864 residual matrix computed from the least squares solution to the multiple linear\\nregression model\\nYij = β0j + β1jAi + β2jSi + εij,\\nwhere Yij is the log-activation of gene j in mouse i, Ai is the age of mouse i, Si indicates if\\n22\\nmouse i is male, εij is a random error, and (β0j, β1j, β2j) is a gene-specific coefficient vector.\\nHere, we are biclustering the residual matrix rather than the raw gene expression matrix\\nbecause we are interested in the structure remaining after adjusting for the observed covariates.\\nThe entries of the residual matrix are not independent (for example, the sum of each\\ncolumn is zero). Also, the responses of many genes are likely correlated with each other. Thus,\\nthe block model required by Theorem 4.1 is not in force, so its conclusion will not hold unless\\nthe dependence between the residuals is negligible. In light of this caveat, the example should\\nbe considered as exploratory data analysis.\\nWe perform biclustering using profile likelihood based on the Gaussian criterion (6.1c)\\nwith 100 random initializations. To determine an appropriate number of mice clusters, K,\\nand gene clusters, L, we experiment with values of K and L between 1 and 15. Figure 4\\npresents the scree plots. The left plot shows that increasing L beyond 5 has a relatively small\\nimpact on the deviance, and similarly, the right plot shows that increasing K beyond 3 has a\\nrelatively minor effect. This suggests we should set K = 3 and L = 5. For this choice of K\\nand L, we experimented with using up to 1000 random starting values, but found no change\\nto the resulting log-likelihood.\\nl l l l l l l l l l l l l l l\\nl\\nl l l l l l l l l l l l l\\nl\\nl l l l l l l l l l l l l\\n2 4 6 8 10 12 14\\n−0.030\\n−0.025\\n−0.020\\n−0.015\\n−0.010\\n−0.005\\n0.000\\nK\\nD\\nev\\nia\\nnc\\ne\\nl l\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nll\\nl\\nll\\nl\\nll\\nl\\nll\\nl\\nll\\nl\\nll\\nl\\nll\\nl\\nll\\nl\\nll\\nl\\nll\\nL\\n2 4 6 8 10 12 14\\n−0.030\\n−0.025\\n−0.020\\n−0.015\\n−0.010\\n−0.005\\n0.000\\nFigure 4: AgeMap likelihood for different values of K and L\\n23\\nFigure 5: Heatmap generated from AGEMAP residual data reflecting the varying expression\\npatterns in the different biclusters. The colors for the matrix entries correspond encode to\\nthe first quartile, the middle two quartiles, and the upper quartile.\\nThe heatmap presented in Figure 7.2 shows the results. The expression levels for gene\\ngroup 1 and 3 appear to be fairly neutral across the three mouse groups, but the other\\nthree gene groups have a more visually apparent pattern. It appears that a mouse can have\\nexpression levels for at most two of gene groups 2, 4, and 5. Mouse group 2 has high expression\\nfor gene groups 2 and 4; mouse group 2 has high expression for gene group 4; and mouse\\ngroup 3 has high expression for gene group 5.\\nWe computed the bicluster assignments found by DI-SIM and KM. The methods failed to\\nconverge after 1000 iterations, but the resulting cluster assignments found by KM and the\\nprofile likelihood method generally agreed, with 89.6% cluster agreement. DI-SIM agreed less\\nwith the other two methods. Between DI-SIM and the profile likelihood method the cluster\\nagreement was 62.5%, and the cluster agreement between DI-SIM and KM was 67.8%.\\nThe three clusters of mice found by the profile-likelihood method also agree with those\\nfound by Perry and Owen (2010). That analysis identified the mouse clusters, but could not\\nattribute meaning to them. The bicluster based analysis has deepened our understanding of\\n24\\nthe three mouse clusters while suggesting some interesting interactions between the genes.\\n8 Discussion\\nWe have developed a statistical setting for studying the performance of biclustering algorithms.\\nUnder the assumption that the data follows a stochastic block model, we derived sufficient\\nconditions for an algorithm based on maximizing a profile-likelihood based criterion function\\nto be consistent. This is the first theoretical guarantee for any biclustering algorithm which\\ncan be applied to a broad range of data distributions and can handle both sparse and dense\\ndata matrices.\\nSince maximizing the criterion function exactly is computationally infeasible, we have\\nproposed an approximate algorithm for obtaining a local optimum rather than a global one.\\nWe have shown through simulations that the approximation algorithm has good performance\\nin practice. Our empirical comparisons demonstrated that the method performs well in a\\nvariety of situations and can outperform existing procedures.\\nApplying the profile-likelihood based biclustering algorithm to real data revealed several\\ninteresting findings. Our results from the GovTrack dataset demonstrated our methods ability\\nto recover ground truth labels when available, and identified motion clusters that were robust\\nacross different methods. Biclustering the genes and mice in the AGEMAP data exposed an\\ninteresting pattern in the expression of certain genes and we found that at most two gene\\ngroups can have high expression levels for any one mouse. The consistency theorem proved in\\nthis report gives conditions under which we can have confidence in the robustness of these\\nfindings.\\n25\\nSupplementary Materials\\nSupplement A Additional Proofs and Theoretical Results\\n(supp-theory.pdf). Rigorous proofs of all theoretical results.\\nSupplement B Additional Empirical and Application Results\\n(supp-empirical.pdf). Additional empirical results for Bernoulli, Gaussian, and Students t\\ndistributed data, as well as an additional application example.\\nReferences\\nArabie, P., Boorman, S. A., and Levitt, P. R. (1978). Constructing blockmodels: How and\\nwhy. J. Math. Pscyh., 17(1):21–63.\\nBickel, P. J. and Chen, A. (2009). A nonparametric view of network models and Newman-\\nGirvan and other modularities. Proc. Nat. Acad. Sci. USA, 106:21068–21073.\\nBrown, L. D. (1986). Fundamentals of Statistical Exponential Families with Applications in\\nStatistical Decision Theory, volume 9 of Lecture Notes – Monograph Series. Institute of\\nMathematical Statistics, Hayward, CA.\\nCheng, Y. and Church, G. M. (2000). Biclustering of expression data. Proceedings International\\nConference on Intelligent Systems for Molecular Biology ; ISMB. International Conference\\non Intelligent Systems for Molecular Biology, 8:93–103.\\nChoi, D., Wolfe, P. J., and Airoldi, E. M. (2012). Stochastic blockmodels with growing number\\nof classes. Biometrika, 99:273–284.\\nDhillon, I. S. (2001). Co-clustering documents and words using bipartite spectral graph\\npartitioning. In Proceedings of the Seventh ACM SIGKDD International Conference on\\nKnowledge Discovery and Data Mining (KDD), pages 26–29, San Francisco.\\n26\\nEisen, M. B., Spellman, P. T., Brown, P. O., and Botstein, D. (1998). Cluster analysis and\\ndisplay of genome-wide expression patterns. Proc. Nat. Acad. Sci. USA, 95(25):14863–14868.\\nGetz, G., Levine, E., and Domany, E. (2000). Coupled two-way clustering analysis of gene\\nmicroarray data. Proc. Nat. Acad. Sci. USA, 97:12079–12084.\\nGolub, G. H. and Loan, C. F. V. (1996). Matrix Computation. Johns Hopkins University\\nPress.\\nHarpaz, R., Perez, H., Chase, H. S., Rabadan, R., Hripcsak, G., and Friedman, C. (2010).\\nBiclustering of Adverse Drug Events in the FDA’s Spontaneous Reporting System. Clinical\\nPharmacology & Therapeutics, 89(2):243–250.\\nHartigan, J. A. (1972). Direct clustering of a data matrix. J. Amer. Statist. Assoc., 67(337):123–\\n129.\\nHofmann, T. (1999). Latent class models for collaborative filtering. In In Proceedings of the\\nSixteenth International Joint Conference on Artificial Intelligence, pages 688–693.\\nHolland, P. W., Laskey, K. B., and Leinhardt, S. (1983). Stochastic blockmodels: First steps.\\nSocial Networks, 5:109–137.\\nKernighan, B. W. and Lin, S. (1970). An Efficient Heuristic Procedure for Partitioning\\nGraphs. The Bell system technical journal, 49(1):291–307.\\nKluger, Y., Basri, R., Chang, J. T., and Gerstein, M. (2003). Spectral biclustering of\\nmicroarray data: Coclustering genes and conditions. Genome Research, 13:703–716.\\nLazzeroni, L. and Owen, A. (2002). Plaid models for gene expression data. Statist. Sinica,\\n12:61–86.\\nMadeira, S. C. and Oliveira, A. L. (2004). Biclustering algorithms for biological data analysis:\\nA survey. IEEE T. Comput. Bi., 1:24–45.\\n27\\nMirkin, B. (1996). Mathematical classification and clustering. Kluwer Academic Press.\\nMurphy, S. A. and van der Vaart, A. W. (2000). On Profile Likelihood. J. Amer. Statist.\\nAssoc., 95(450):449–465.\\nNewman, M. E. J. (2006). Modularity and community structure in networks. Proc. Nat. Acad.\\nSci. USA, 103(23):8577–8582.\\nPerry, P. O. and Owen, A. B. (2010). A rotationn test to verify latent structure. J. Mach.\\nLearn. Res., 11:603–624.\\nRohe, K., Chatterjee, S., and Yu, B. (2011). Spectral clustering and the high-dimensional\\nstochastic blockmodel. Ann. Statist., 39(4):1878–1915.\\nRohe, K. and Yu, B. (2012). Co-clustering for directed graphs; the stochastic co-blockmodel\\nand a spectral algorithm. Preprint arXiv:1204.2296.\\nSeldin, Y. and Tishby, N. (2009). Pac-bayesian generalization bound for density estimation\\nwith application to co-clustering. In 12th International Conference on Artificial Intelligence\\nand Statistics (AISTATS).\\nSeldin, Y. and Tishby, N. (2010). Pac-bayesian analysis of co-clustering and beyond. Journal\\nof Machine Learning Research, 11:3595–3646.\\nTarpey, T. and Flury, B. (1996). Self-consistency: A fundamental concept in statistics. Statist.\\nSci., 11(3):229–243.\\nUngar, L. and Foster, D. P. (1998). A formal statistical approach to collaborative filtering. In\\nCONALD98.\\nVaradhan, S. R. S. (2001). Probability Theory (Courant Lecture Notes). American Mathemat-\\nical Soceity.\\n28\\nZahn, J. M., Poosala, S., Owen, A. B., Ingram, D. K., Lustig, A., Carter, A., Weeraratna,\\nA. T., Taub, D. D., Gorospe, M., Mazan-Mamczarz, K., Lakatta, E. G., Boheler, K. R.,\\nXu, X., Mattson, M. P., Falco, G., Ko, M. S. H., Schlessinger, D., Firman, J., Kummerfeld,\\nS. K., Wood, W. H., Zonderman, A. B., Kim, S. K., and Becker, K. G. (2007). AGEMAP:\\nA Gene Expression Database for Aging in Mice. PLOS Genetics.\\nZhao, Y., Levina, E., and Zhu, J. (2011). Community extraction for social networks. P. Natl.\\nAcad. Sci. USA, 108:7321–7326.\\nZhao, Y., Levina, E., and Zhu, J. (2012). Consistency of community detection in networks\\nunder degree-corrected stochastic block models. Ann. Stat., 40(4):2266–2292.\\n29\\nConsistent Biclustering: Additional Proofs and\\nTheoretical Results\\nCheryl J. Flynn and Patrick O. Perry\\nNew York University\\nNovember 17, 2015\\nA Proof of Formal Consistency Theorem\\nOur proof of the main theoretical results follows the same outline as Section 3.\\nIn particular, Proposition A.2 is a rigorous statement analogous to Proposi-\\ntion 3.1; Proposition A.3 is analogous to Proposition 3.2; Theorems 4.1 and 4.2\\nare analogous to Proposition 3.3.\\nA.1 Population criterion\\nHere, we give a rigorous statement of Proposition 3.1. That is, we establish\\nthat in the limit, F is close to its nonrandom population version, G, which\\ndepends only on the confusion matrices.\\n1\\nWe will need some additional notation and a concentration result. Define\\nnormalized residual matrix R(g,h) ∈ RK×L by\\nR(g,h) = ρ−1{X¯(g,h)−E(g,h)}.\\nThe law of large numbers establishes that for fixed g and h, the convergence\\nRkl(g,h)\\nP→ 0 holds. We can prove a stronger concentration result, that this\\nconvergence is uniform over all g and h.\\nLemma A.1. Under conditions (C1)–(C6), for all ε > 0,\\nsup\\nJε\\n‖R(g,h)‖∞ P→ 0,\\nwhere ‖A‖∞ = maxk,l |Akl| for any matrix A.\\nProof. For all t > 0,\\nPr\\n(\\nsup\\nJε\\n‖R(g,h)‖∞ > t\\n)\\n≤ KLPr\\n(\\nsup\\nI∈In\\nρ−1\\n∣∣∣ ∑\\n{i,j}∈I\\n(Xij − µcidj)\\n∣∣∣ > t|I|),\\nwhere In ⊂ 2[n]× 2[m] is the set of all biclusters such that pˆk > ε for all k and\\nqˆl > ε for all l. Since In is a subset of the power set 2[nm], by Lemma B.1 in\\nAppendix B, it follows that the right hand side tends to zero.\\nWith Lemma A.1, we can establish that in the limit, F (g,h) is close to\\nG(C,D).\\n2\\nProposition A.2. F is close to its population version in the sense that, for\\nall ε > 0,\\nsup\\nJε\\n|F (g,h)−G(C,D)| P→ 0.\\nProof. The technical assumptions of f imply that its first derivative is\\nbounded. Therefore, f is locally Lipschitz continuous with Lipschitz constant\\nc = sup |f ′(µ)| for µ in a neighborhood of M and\\n|F (g,h)−G(C,D)| ≤ c‖R(g,h)‖∞.\\nFrom Proposition A.1, the right hand side converges to zero almost surely\\nand the result follows.\\nA.2 Self-consistency\\nOnce we have established that F is close to its population version, our next\\ntask is to show that the population version is self-consistent. We will need a\\nmore precise version of Proposition 3.2.\\nTo state the result, for δ > 0, define\\nPδ = {C ∈ Cp : max\\na6=a′\\nCakCa′k < δ}, (A.1a)\\nQδ = {D ∈ Cq : max\\nb 6=b′\\nDblDb′l < δ}. (A.1b)\\nA permutation of a diagonal matrix has only one non-zero entry in each\\ncolumn, so taking δ close to zero restricts the confusion matrices to be close\\n3\\nto permutations of diagonal matrices.\\nProposition A.3. If mina{pa} > η, minb{qb} > η, and (C,D) /∈ Pδ ×Qδ,\\nthen G(C,D) is small, in the sense that\\nG(C,D)−\\n∑\\na,b\\npa qb f(ρ\\n−1µab) ≤ −κη2δ,\\nwhere κ is a constant independent of δ and η.\\nProof. If D /∈ Qδ, then for some l and some b 6= b′, DblDb′l ≥ δ. Since no\\ntwo columns of M are identical, there exists an a such that µab 6= µab′ . Let k\\nbe the index of the largest element in row a of matrix C; this element must\\nbe at least as large as the mean, i.e.\\nCak ≥ [C1]a\\nK\\n≥ η\\nK\\n.\\nLet W = [CT1]k[D\\nT1]l; this is nonzero. Now, there exists µ∗ ∈M such that\\n[CTMD]kl = CakDblµab + CakDb′lµab′ + (W − CakDbl − CakDb′l)µ∗.\\nLet z = [CTMD]kl/W . Set κ0 = infµ∈M f ′′(µ) and define N = [νab] ∈ RA×B\\nwith νab = f(ρ\\n−1µab). By a refined Jensen’s inequality (Lemma B.2 in\\n4\\nAppendix B), it follows that\\n[CTND]kl\\nW\\n− f(z) ≥ κ0C\\n2\\nakDblDb′l\\nW 2\\n(1\\n2\\n(µab − z)2 + 1\\n2\\n(z − µab′)2\\n)\\n≥ κ0C\\n2\\nakDblDb′l\\nW 2\\n(1\\n2\\n(µab − z) + 1\\n2\\n(z − µab′)\\n)2\\n= κ0\\nC2akDblDb′l\\n4W 2\\n(µab − µab′)2.\\nThus\\n[CT1]k[D\\nT1]lf\\n(\\n[CTM 0D]kl\\n[CT1]k[D\\nT1]l\\n)\\n− [CTND]kl\\n≤ −κ0\\n2\\n(µab − µab′)2C\\n2\\nakDblDb′l\\nW\\n≤ −κ0η\\n2δ\\n4K2\\n(µab − µab′)2.\\nThis inequality only holds for one particular choice of k and l; for other\\nchoices, the left hand side is nonpositive by Jensen’s inequality. Defining\\nκ1 =\\nκ0\\n4\\nmin\\na,b 6=b′\\n(µab − µab′)2,\\nit follows that\\nG(C,D)−\\n∑\\na,b\\npa qb f(ρ\\n−1µab) ≤ −κ1 η\\n2δ\\nK2\\n.\\nSimilarly, if C /∈ Pδ, then the right hand side is bounded by\\n−κ2 η2δ/L2\\n5\\nwhere\\nκ2 =\\nκ0\\n4\\nmin\\na6=a′,b\\n(µab − µa′b)2.\\nThe result of the proposition follows with κ = min(κ1, κ2)/max{K2, L2}.\\nA.3 Consistency\\nWe are now ready to state and prove the formal consistency theorem.\\nProof of Theorem 4.1. Fix δ > 0 and define Pδ and Qδ as in (A.1). We will\\nshow that if (g,h) ∈ Jε and if (C(g),D(h)) /∈ (Pδ,Qδ), then F (g,h) <\\nF (c,d) with probability approaching one. Moreover, this inequality holds\\nuniformly over all such choices of (g,h). Since δ is arbitrary, this implies\\nthat C(gˆ) and D(hˆ) converge to permutations of diagonal matrices, i.e. the\\nproportions of misclassified rows and columns converge to zero.\\nSet rn = supJε |F (g,h) − G(C(g),D(h)|. Suppose (g,h) ∈ Jε. In this\\ncase,\\nF (g,h)− F (c,d) ≤ 2rn + {G(C(g),D(h))−G(C(c),D(d))}\\n= 2rn +\\n{\\nG(C(g),D(h))−\\n∑\\na,b\\n[C1]a[D1]bf([M 0]ab)\\n}\\n.\\nPick η > 0 smaller than mina{pa} and minb{qb}. By assumption, the true\\nrow and column class proportions converge to p and q. Thus, for all g ∈ Km\\nand h ∈ Ln, for n large enough, [C(g)]a ≥ η and [D(h)]b ≥ η; this holds\\nuniformly over all choices of (g,h).\\n6\\nApplying Proposition A.3, to the second term in the inequality, we get\\nthat with probability approaching one,\\nF (g,h)− F (c,d) ≤ 2rn − κη2δ\\nfor all (g,h) ∈ Jε such that (C(g),D(h)) /∈ Pδ ×Qδ. By Proposition A.2,\\nrn\\nP→ 0. Thus, with probability approaching one, (C(gˆ),D(hˆ)) ∈ Pδ ×Qδ.\\nSince this result holds for all δ, all limit points of C(gˆ) and D(hˆ) must be\\npermutations of diagonal matrices.\\nA.4 Empirical treatment of ρ\\nFor the Poisson and Gaussian relative entropy functions (6.1b) and (6.1c),\\nthe maximizer of the criterion function (2.2) does not depend on the scale\\nfactor ρ. This is immediately obvious in the Gaussian case. For the Poisson\\ncase, the function fPoisson(µ/ρ) =\\n1\\nρ\\nµ log(µ)− 1\\nρ\\nµ(1 + log(ρ)). When summed\\nover all biclusters, the second term in this sum is equal to a constant so the\\nvalue of µ which maximizes fPoisson(µ/ρ) does not depend on the value of ρ.\\nThis is not the case for the Binomial relative entropy function (6.1a), but\\nthe parameter ρ is not identifiable in practice so it does not make sense to\\ntry to estimate it. For our simulations we use which maximizes ρ = 1 in\\nthe fitting procedure, regardless of the true scale factor for the mean matrix\\nM . Even though in the simulations the identifiability condition doesn’t hold\\nfor this choice of ρ, we still get consistency, because the maximizer of the\\n7\\ncriterion with fBernoulli(µ) is close to the maximizer with fPoisson(µ). See Perry\\nand Wolfe (2012) for discussion of a related phenomenon.\\nB Additional Technical Results\\nLemma B.1. For each n, let Xn,m, 1 ≤ m ≤ n, be independent random\\nvariables with EXn,m = 0. Let ρn be a sequence of positive numbers. Let In\\nbe a subset of the powerset 2[n], with inf{|I| : I ∈ In} ≥ Ln. Suppose\\n(i) 1\\nnρn\\n∑n\\nm=1 E|Xn,m|2 is uniformly bounded in n;\\n(ii) For all ε > 0, 1\\nnρ2n\\n∑n\\nm=1 E(|Xn,m|2; |Xn,m| > ε\\n√\\nnρn)→ 0;\\n(iii) limn→∞ nLn <∞;\\n(iv) limn→∞\\nlog|In|√\\nn\\n<∞.\\n(v) limn→∞ ρn\\n√\\nn =∞.\\nThen\\nsup\\nI∈In\\n∣∣∣ 1\\nρn|I|\\n∑\\nm∈I\\nXn,m\\n∣∣∣ P→ 0.\\nProof. Let ε > 0 be arbitrary. Define Yn,m = ρ\\n−1\\nn Xn,m I(|Xn,m| ≤ ε\\n√\\nnρn),\\nand note that\\nPr(Yn,m 6= ρ−1n Xn,m for some 1 ≤ m ≤ n) ≤\\nn∑\\nm=1\\nPr(|Xn,m| > ε\\n√\\nnρn)\\n≤ 1\\nε2nρ2n\\nn∑\\nm=1\\nE(|Xn,m|2; |Xn,m| > ε\\n√\\nnρn).\\n8\\nFix an arbitrary t > 0. Set µn,m = EYn,m and for I ∈ In define\\nµn(I) =\\n1\\n|I|\\n∑\\nm∈I\\nµn,m.\\nFor I ∈ In, write\\nPr\\n(∑\\nm∈I\\nYn,m > t |I|\\n)\\n= Pr\\n(∑\\nm∈I\\n(Yn,m − µn,m) > |I|\\n(\\nt− µn(I)\\n))\\n.\\nNote that since EXn,m = 0, it follows that\\n|µn,m| = |−E(ρ−1n Xn,m; |Xn,m| > ε\\n√\\nnρn)|\\n≤ 1\\nε\\n√\\nnρ2n\\nE(|Xn,m|2; |Xn,m| > ε\\n√\\nnρn).\\nThus, by (ii) and (iii) we have that supI∈In{|µn(I)|} → 0; in particular,\\nfor n large enough, supI∈In{|µn(I)|} < t2 . Consequently, for n large enough,\\nuniformly for all I,\\nPr\\n(∑\\nm∈I\\nYn,m > t |I|\\n)\\n≤ Pr\\n(∑\\nm∈I\\n(Yn,m − µn,m) > t |I|/2\\n)\\n.\\nSimilarly,\\nPr\\n(∑\\nm∈I\\nYn,m < −t |I|\\n)\\n≤ Pr\\n(∑\\nm∈I\\n(Yn,m − µn,m) < −t |I|/2\\n)\\n.\\nWe apply Bernstein’s inequality to the bound. Define σ2n,m = E(Yn,m −\\n9\\nµn,m)\\n2 and vn(I) =\\n∑\\nm∈I σ\\n2\\nn,m. Note that |Yn,m − µn,m| ≤ 2ε\\n√\\nn. By Bern-\\nstein’s inequality,\\nPr\\n(∣∣∣∑\\nm∈I\\n(Yn,m − µn,m)\\n∣∣∣ > t |I|/2) ≤ 2 exp{− t2|I|2/8\\nvn(I) + εt|I|\\n√\\nn/3\\n}\\n.\\nBy (i), (iv), and (v), it follows that for n large enough, vn(I) < εt|I|\\n√\\nn/3, so\\nPr\\n(∣∣∣ ∑\\nm∈In\\n(Yn,m − µn,m)\\n∣∣∣ > t|I|/2) ≤ 2 exp{− t|I|\\n8ε\\n√\\nn\\n}\\n.\\nWe use this bound for each I to get the union bound:\\nPr\\n(\\nsup\\nI∈In\\n∣∣∣ 1|I|∑\\nm∈I\\nYn,m\\n∣∣∣ > t) ≤ 2|In| exp{− tLn\\n8ε\\n√\\nn\\n}\\n= 2 exp\\n{\\nlog|In| − tLn\\n8ε\\n√\\nn\\n}\\n.\\nBy (iii) and (iv), it is possible to choose ε such that the right hand side goes\\nto zero. It follows then that\\nPr\\n(\\nsup\\nI∈In\\n∣∣∣ 1\\nρn|I|\\n∑\\nm∈I\\nXn,m\\n∣∣∣ > t) ≤ Pr(Yn,m 6= ρ−1n Xn,m for some 1 ≤ m ≤ n)\\n+ Pr\\n(\\nsup\\nI∈In\\n∣∣∣ 1|I|∑\\nm∈I\\nYn,m\\n∣∣∣ > t)\\n→ 0.\\n10\\nLemma B.2 (Refined Jensen’s Inequality). Let f : R→ R be twice differen-\\ntiable and let N be a convex set in R. If x1, . . . , xn are points in N , and if\\nw1, . . . , wn are nonnegative numbers summing to one, then\\nn∑\\ni=1\\nwif(xi)− f(z) ≥ 1\\n2\\ninf\\ny∈N\\nf ′′(y)\\nn∑\\ni=1\\nwi(xi − z)2,\\nwhere z =\\n∑n\\ni=1wixi.\\nProof. Define κ0 = infy∈N f ′′(y) and use the bound\\nf(xi) ≥ f(z) + f ′(z)(xi − z) + κ0\\n2\\n(xi − z)2.\\nC Finite Sample Results\\nIn this appendix we derive a finite sample tail bound for the probability that\\nthe class assignments that maximize the profile likelihood are close to the\\ntrue class labels. To proceed in this setting, we make stronger distributional\\nassumptions than in the asymptotic case. Specifically, we assume here that\\nthe entries Xij|c,d follow a Gaussian distribution with mean µcidj and finite\\nvariance σ2. We proceed with the notation from the main text.\\n11\\nProposition C.1. For all ε > 0, if t < σ then\\nPr\\n(\\nsup\\nJε\\n‖R(g,h)‖∞ > t\\n)\\n≤ 2Km+1Ln+1 exp\\n(\\n− Lnt\\n2\\n4σ2\\n)\\n,\\nand if t ≥ σ then\\nPr\\n(\\nsup\\nJε\\n‖R(g,h)‖∞ > t\\n)\\n≤ 2Km+1Ln+1 exp\\n(\\n− Lnt\\n4σ\\n)\\nwhere ‖A‖∞ = maxk,l |Akl| for any matrix A.\\nProof. If the entries Xij follow a Gaussian distribution with mean µcidj and\\nvariance σ2 then\\nE\\n(\\n|Xij − µcidj |l\\n)\\n≤ σ\\n2\\n2\\nσl−2l!\\nso the conditions of Bernstein’s inequality hold. It follows that for any\\nbicluster I, for all t > 0,\\nPr\\n(\\n|\\n∑\\ni,j∈I\\nXij − µcidj | > t|I|\\n)\\n≤ 2 exp\\n{\\n− |I|\\n2t2\\n2(σ2|I|+ σ|I|t)\\n}\\n≤ 2 exp\\n{\\n− Lnt\\n2\\n4 max{σ2, σt}\\n}\\n.\\nApplying a union bound,\\nPr\\n(\\nsup\\nJε\\n‖R(g,h)‖∞ > t\\n)\\n≤ 2Km+1Ln+1 exp\\n{\\n− Lnt\\n2\\n4 max{σ2, σt}\\n}\\n.\\n12\\nProposition C.1 is used to establish a finite sample bound on the difference\\nbetween F (g,h) and its population version.\\nProposition C.2. Under conditions (C2) and (C3), for any t > 0,\\nPr\\n(\\nsup\\nJε\\n|F (g,h)−G(C,D)| > t\\n)\\n≤ Pr\\n(\\nsup\\nJε\\n‖R(g,h)‖∞ > t\\nc\\n)\\nwhere c = sup |f ′(µ)| for µ in M.\\nProposition C.2 is a direct consequence of the fact that f is locally Lipschitz\\ncontinuous under conditions (C2) and (C3). The details are similar to proof\\nof Proposition A.2.\\nThe next step is to show that population version is maximized at the true\\nclass labels.\\nProposition C.3. Choose τ > 0 such that mina6=a′,b(µab − µa′b)2 ≥ τ and\\nmina,b 6=b′(µab − µab′)2 ≥ τ . Then for all ε > 0, for (g,h) ∈ Jε and (C,D) /∈\\nPδ ∩Qδ, G(C,D) is small in the sense that\\nG(C,D)−\\n∑\\na,b\\npaqbf(ρ\\n−1µab) ≤ − τε\\n2δ\\n4 max{K2, L2} .\\nThe proof of Proposition C.3 is similar to the proof of Proposition A.3\\nexcept that the bound on the difference uses ε in place of the value η. Some\\ndetails follow.\\nProof of Proposition C.3. First note that in Proposition A.3, we can let a be\\nthe index of the largest element in column k of matrix C; then, since we are\\n13\\nrestricted to the set Jε, this element must be at least as large as\\nCak ≥ [C\\nT1]k\\nK\\n≥ ε\\nK\\n.\\nNoting that for the Gaussian relative entropy function f ′′(µ) = 1 for all µ ∈M,\\nthe remainder of the proof is similar to the proof of Proposition A.3.\\nWe establish a finite sample bound by combining these results.\\nProof. Proof of Theorem 4.2 Fix δ > 0 and define Pδ and Qδ as in Proposi-\\ntion A.3. Set rn = supJε |F (g,h)−G(C(g),D(h)|. Suppose (g,h) ∈ Jε. In\\nthis case,\\nF (g,h)− F (c,d) ≤ 2rn + {G(C(g),D(h))−G(C(c),D(d))}\\n= 2rn +\\n{\\nG(C(g),D(h))−\\n∑\\na,b\\n[C1]a[D1]bf([M 0]ab)\\n}\\n.\\nApplying Proposition C.3 to the second term in the inequality, we get that\\nF (g,h)− F (c,d) ≤ 2rn − τε\\n2δ\\n4 max{K2, L2}\\nfor all (g,h) ∈ Jε such that (C,D) /∈ Pδ∩Qδ. The result follows by applying\\nProposition C.2.\\n14\\nConsistent Biclustering: Additional Empirical\\nResults\\nCheryl J. Flynn and Patrick O. Perry\\nNew York University\\nNovember 17, 2015\\nThis supplementary appendix reports additional empirical results for\\nBernoulli, Gaussian, and Student’s t distributed data as well as an additional\\napplication example.\\n1 Additional Empirical Results\\nFigures 1-3 present the average bicluster misclassification rates for each sample\\nsize and Tables 1-3 report the standard deviations for the Bernoulli, Gaussian,\\nand t simulations, respectively. Since the normalization for the DI-SIM\\nalgorithm is only specified for non-negative data, the algorithm is run on\\nthe un-normalized matrix for the Gaussian and non-standardized Student’s t\\nexamples.\\n1\\nFor the Bernoulli simulation, we simulate from a block model with K = 2\\nrow clusters and L = 3 column clusters. We vary the number of columns,\\nn, between 200 to 1400 and we take the number of rows as m = γn where\\nγ ∈ {0.5, 1, 2}.\\nWe set the row and column class membership probabilities as p = (0.3, 0.7)\\nand q = (0.2, 0.3, 0.5). We choose the matrix of block parameters to be\\nM =\\nb√\\nn\\n\\uf8eb\\uf8ec\\uf8ed0.43 0.06 0.13\\n0.10 0.34 0.17\\n\\uf8f6\\uf8f7\\uf8f8 .\\nwhere the entries were selected to be on the same scale as Bickel and Chen\\n(2009). We vary b between 5 and 20. We generate the data conditional on\\nthe row and column classes as Xij | c,d ∼ Bernoulli(µcidj). We initialize all\\nthree methods with 250 random starting values.\\n2\\nll\\nl\\nl l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n0\\n0.\\n2\\n0.\\n4\\n0.\\n6\\nm = 0.5n, b = 5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl l l l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n0\\n0.\\n2\\n0.\\n4\\n0.\\n6\\nm = 0.5n, b = 10\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n0\\n0.\\n2\\n0.\\n4\\n0.\\n6\\nm = 0.5n, b = 20\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl l l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n0\\n0.\\n1\\n0.\\n2\\n0.\\n3\\n0.\\n4\\nm = 1n, b = 5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n0\\n0.\\n1\\n0.\\n2\\n0.\\n3\\n0.\\n4\\nm = 1n, b = 10\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n0\\n0.\\n1\\n0.\\n2\\n0.\\n3\\n0.\\n4\\nm = 1n, b = 20\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl l l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n00\\n0.\\n10\\n0.\\n20\\nm = 2n, b = 5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n00\\n0.\\n10\\n0.\\n20\\nm = 2n, b = 10\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l l l\\n200 400 600 800 1000 1200 1400\\n0.\\n00\\n0.\\n10\\n0.\\n20\\nm = 2n, b = 20\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl PL KM DI−SIM\\nFigure 1: Average misclassification rates for Bernoulli example over 100\\nsimulations.\\nm = 0.5n\\nPL KM DS\\nn b=5 b=10 b=20 b=5 b=10 b=20 b=5 b=10 b=20\\n200 0.0834 0.0268 0.0075 0.1141 0.0515 0.0089 0.0760 0.0440 0.0249\\n400 0.0473 0.0145 0.0035 0.0932 0.0168 0.0051 0.0400 0.0346 0.0139\\n600 0.0224 0.0089 0.0021 0.0654 0.0118 0.0027 0.0347 0.0196 0.0075\\n800 0.0168 0.0061 0.0018 0.0266 0.0069 0.0024 0.0273 0.0173 0.0048\\n1000 0.0118 0.0049 0.0010 0.0156 0.0055 0.0014 0.0218 0.0134 0.0033\\n1200 0.0094 0.0041 0.0007 0.0122 0.0052 0.0011 0.0205 0.0114 0.0031\\n1400 0.0079 0.0031 0.0006 0.0100 0.0038 0.0010 0.0161 0.0089 0.0023\\nm = n\\nn b=5 b=10 b=20 b=5 b=10 b=20 b=5 b=10 b=20\\n200 0.0528 0.0125 0.0020 0.1084 0.0181 0.0031 0.0514 0.0352 0.0104\\n400 0.0192 0.0053 0.0006 0.0265 0.0080 0.0012 0.0348 0.0147 0.0034\\n600 0.0114 0.0027 0.0003 0.0156 0.0049 0.0007 0.0248 0.0097 0.0018\\n800 0.0071 0.0021 0.0002 0.0107 0.0032 0.0003 0.0179 0.0064 0.0011\\n1000 0.0052 0.0013 0.0000 0.0073 0.0022 0.0002 0.0140 0.0046 0.0008\\n1200 0.0043 0.0008 0.0000 0.0067 0.0017 0.0001 0.0120 0.0033 0.0005\\n1400 0.0034 0.0008 0.0000 0.0057 0.0015 0.0001 0.0097 0.0026 0.0005\\nm = 2n\\nn b=5 b=10 b=20 b=5 b=10 b=20 b=5 b=10 b=20\\n200 0.0197 0.0037 0.0000 0.0505 0.0076 0.0000 0.0388 0.0112 0.0025\\n400 0.0062 0.0009 0.0000 0.0120 0.0026 0.0000 0.0212 0.0044 0.0003\\n600 0.0036 0.0004 0.0000 0.0073 0.0016 0.0000 0.0103 0.0025 0.0000\\n800 0.0025 0.0003 0.0000 0.0049 0.0008 0.0000 0.0065 0.0014 0.0001\\n1000 0.0015 0.0000 0.0000 0.0035 0.0005 0.0000 0.0045 0.0011 0.0001\\n1200 0.0012 0.0001 0.0000 0.0029 0.0003 0.0000 0.0041 0.0007 0.0000\\n1400 0.0009 0.0000 0.0000 0.0024 0.0002 0.0000 0.0032 0.0006 0.0000\\nTable 1: Standard deviations for Bernoulli example over 100 simulations.\\nFor the Gaussian simulation, we simulate from a block model with K = 2\\nrow clusters and L = 3 column clusters. We vary the number of columns,\\n3\\nn, between 50 to 400 and we take the number of rows as m = γn where\\nγ ∈ {0.5, 1, 2}.\\nWe set the row and column class membership probabilities as p = (0.3, 0.7)\\nand q = (0.2, 0.3, 0.5). We choose the matrix of block parameters to be\\nM = b\\n\\uf8eb\\uf8ec\\uf8ed 0.47 0.15 −0.60\\n−0.26 0.82 0.80\\n\\uf8f6\\uf8f7\\uf8f8\\nwhere the entries were simulated from a uniform distribution on [−1, 1]. We\\nvary b between 0.5 and 2. We generate the data conditional on the row and\\ncolumn classes as Xij | c,d ∼ Gaussian(µcidj , σ = 1). We initialize all three\\nmethods with 100 random starting values.\\nl\\nl\\nl\\nl l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 0.5n, b = 0.5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 0.5n, b = 1\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 0.5n, b = 2\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 1n, b = 0.5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 1n, b = 1\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 1n, b = 2\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 2n, b = 0.5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 2n, b = 1\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 2n, b = 2\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl PL KM DI−SIM\\nFigure 2: Average misclassification rates for Gaussian example over 500\\nsimulations.\\n4\\nm = 0.5n\\nPL KM DS\\nn b=0.5 b=1 b=2 b=0.5 b=1 b=2 b=0.5 b=1 b=2\\n50 0.1142 0.0718 0.0306 0.0928 0.0923 0.0491 0.1028 0.1815 0.0669\\n100 0.0610 0.0345 0.0051 0.0657 0.0675 0.0074 0.1368 0.1085 0.0073\\n200 0.0329 0.0127 0.0005 0.0722 0.0181 0.0005 0.1598 0.0187 0.0006\\n300 0.0235 0.0052 0.0000 0.0552 0.0068 0.0000 0.0809 0.0071 0.0000\\n400 0.0169 0.0023 0.0000 0.0363 0.0027 0.0000 0.0345 0.0030 0.0000\\nm = n\\nn b=0.5 b=1 b=2 b=0.5 b=1 b=2 b=0.5 b=1 b=2\\n50 0.0968 0.0503 0.0080 0.0856 0.0975 0.0122 0.1133 0.1804 0.0121\\n100 0.0441 0.0175 0.0008 0.0710 0.0343 0.0010 0.1635 0.0467 0.0009\\n200 0.0235 0.0030 0.0000 0.0629 0.0046 0.0000 0.0993 0.0046 0.0000\\n300 0.0124 0.0007 0.0000 0.0285 0.0011 0.0000 0.0274 0.0013 0.0000\\n400 0.0075 0.0002 0.0000 0.0132 0.0003 0.0000 0.0139 0.0004 0.0000\\nm = 2n\\nn b=0.5 b=1 b=2 b=0.5 b=1 b=2 b=0.5 b=1 b=2\\n50 0.0783 0.0235 0.0013 0.0795 0.0753 0.0013 0.1273 0.1161 0.0013\\n100 0.0323 0.0040 0.0000 0.0839 0.0086 0.0000 0.1684 0.0105 0.0000\\n200 0.0106 0.0004 0.0000 0.0310 0.0006 0.0000 0.0323 0.0007 0.0000\\n300 0.0045 0.0000 0.0000 0.0085 0.0000 0.0000 0.0104 0.0000 0.0000\\n400 0.0020 0.0000 0.0000 0.0041 0.0000 0.0000 0.0047 0.0000 0.0000\\nTable 2: Standard deviations for Gaussian example over 500 simulations.\\nFor the non-standardized Student’s t simulation, we use the same parame-\\nters as in the Gaussian simulation and we generate the data conditional on\\nthe row and column classes as Xij | c,d ∼ t(µcidj , σ = 1) with four degrees of\\nfreedom. We initialize all three methods with 100 random starting values.\\nl\\nl\\nl\\nl l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 0.5n, b = 0.5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 0.5n, b = 1\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 0.5n, b = 2\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl\\nl l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 1n, b = 0.5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 1n, b = 1\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 1n, b = 2\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl\\nl l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 2n, b = 0.5\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl\\nl l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 2n, b = 1\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl l l l l\\n50 100 150 200 250 300 350 400\\n0.\\n0\\n0.\\n4\\n0.\\n8\\nm = 2n, b = 2\\nn\\nAv\\ng \\nM\\nisc\\nla\\nss\\nific\\nat\\nio\\nn \\nRa\\nte\\nl PL KM DI−SIM\\nFigure 3: Average misclassification rates for t example over 500 simulations.\\n5\\nm = 0.5n\\nPL KM DS\\nn b=0.5 b=1 b=2 b=0.5 b=1 b=2 b=0.5 b=1 b=2\\n50 0.0958 0.1018 0.0526 0.0792 0.1028 0.1176 0.0730 0.1316 0.1784\\n100 0.0877 0.0485 0.0175 0.0884 0.0871 0.0775 0.0884 0.1505 0.1045\\n200 0.0429 0.0233 0.0040 0.0620 0.0892 0.0430 0.1119 0.1493 0.0582\\n300 0.0298 0.0137 0.0013 0.0620 0.0831 0.0322 0.1171 0.1245 0.0355\\n400 0.0235 0.0093 0.0007 0.0640 0.0769 0.0192 0.1261 0.1082 0.0328\\nm = n\\nn b=0.5 b=1 b=2 b=0.5 b=1 b=2 b=0.5 b=1 b=2\\n50 0.1073 0.0724 0.0229 0.0798 0.0895 0.1034 0.0777 0.1430 0.1306\\n100 0.0690 0.0344 0.0052 0.0856 0.1021 0.0633 0.1091 0.1581 0.0942\\n200 0.0324 0.0108 0.0009 0.0659 0.0979 0.0298 0.1208 0.1380 0.0370\\n300 0.0220 0.0048 0.0003 0.0668 0.0706 0.0194 0.1292 0.0897 0.0272\\n400 0.0155 0.0024 0.0002 0.0827 0.0556 0.0134 0.1411 0.0685 0.0088\\nm = 2n\\nn b=0.5 b=1 b=2 b=0.5 b=1 b=2 b=0.5 b=1 b=2\\n50 0.0976 0.0514 0.0086 0.0843 0.1057 0.0757 0.0966 0.1550 0.1000\\n100 0.0528 0.0147 0.0007 0.0774 0.1141 0.0387 0.1258 0.1498 0.0578\\n200 0.0211 0.0035 0.0002 0.0752 0.0807 0.0226 0.1289 0.0970 0.0333\\n300 0.0114 0.0009 0.0000 0.0989 0.0469 0.0130 0.1263 0.0579 0.0002\\n400 0.0068 0.0003 0.0000 0.1086 0.0361 0.0000 0.1385 0.0412 0.0001\\nTable 3: Standard deviations for t example over 500 simulations.\\nSimilar to the Poisson simulation, biclustering based on the profile log-\\nlikelihood criterion performs at least as well as the other methods and shows\\nsigns of convergence in all three examples. These results provide further\\nverification of the theoretical findings and support the use of biclustering\\nbased on the profile log-likelihood criterion.\\n2 Additional Application - MovieLens\\nSince consumer habits likely vary depending on products, biclustering review-\\nwebsite data can help identify structure in the data and identify groups of\\nconsumers and groups of products with similar patterns. As an application\\nof this we apply biclustering to the MovieLens dataset (?).\\nThe MovieLens dataset consists of 100,000 movie reviews on 1682 movies\\nby 943 users. Each user has rated at least 20 movies and each movie is rated\\non a scale from one to five. In addition to the review rating, the release date\\n6\\nand genre of each movie is available as well as some demographic information\\nabout each user including gender, age, occupation and zip code.\\nIn order to retain customers, movie-renting services strive to recommend\\nnew movies to individuals who are likely to view them. Since most users only\\nreview movies that they have already seen, we can use the structure of the\\nuser-movie review matrix to identify associations between users and viewing\\nhabits of movies. Specifically, we consider the 943×1682 binary matrix X\\nwhere Xij = 1 if user i has rated movie j and Xij = 0 otherwise. To find\\nstructure in X, we biclustered the rows and columns of X using the profile\\nlikelihood based on the Bernoulli criterion (6.1a).\\nTo determine a reasonable selection for the number of biclusters we varied\\nthe number of user groups, K, and the number of movie groups, L, each from\\n1 to 10. For each combination of K and L, we computed the optimal cluster\\nassignments based on 250 random starting values. Figure 4 presents the scree\\nplots as functions of K and L. From the left scree plot, we see little change\\nto the deviance when increasing L beyond 4. From the right scree plot, the\\ndeviance does not decrease much beyond when increasing K beyond 3. Based\\non these two plots, we set K = 3 and L = 4. For K = 3 and L = 4, we\\nexperimented with using up to 2000 random starting values, but found no\\nchange to the resulting log-likelihood.\\n7\\nll\\nl l l l l l l l\\nl\\nl\\nl\\nl\\nl l l l l l\\n2 4 6 8 10\\n0.30\\n0.35\\n0.40\\n0.45\\nK\\nD\\nev\\nia\\nnc\\ne\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nL\\n2 4 6 8 10\\n0.30\\n0.35\\n0.40\\n0.45\\nFigure 4: MovieLens likelihood for different values of K and L\\nFigure 5: Heatmap generated from MovieLens data reflecting the varying\\nreview patterns in the different biclusters. Blue identifies movies with no\\nreview and white identifies rated movies.\\n8\\nWe compared the resulting bicluster assignments to those found by Di-Sim\\nand K-Means. The cluster assignments varied between the three methods, with\\nthe least amount of disagreement between K-Means and the profile-likelihood\\nmethod (55.2% of cluster assignments agreed) and the most disagreement\\nbetween Di-Sim and the profile-likelihood method (24.2% of clusters agreed).\\nFigure 5 presents the heatmap of the data based on the resulting bicluster\\nassignments from the profile-likelihood method, with the ordering of the\\nclusters determined by the total number of a reviews in each cluster. Roughly\\nspeaking, user group 3 is consistently active across all movie groups with\\nincreasing activity as the popularity of the movie increases. The reviewing\\nhabits of user group 2 follow a similar pattern but to a lesser extent. In\\ncontrast, user group 1 is consistently inactive with the only exceptions being\\nmovie group 4.\\nThe median ages within the user group were 33, 30, and 29, and the\\npercentages of male users within each group were 68.7%, 72.8%, and 77.4%.\\nThese statistics suggest that there is some age and gender effect on the\\nreviewing habits of the users.\\nTable 4 reports the top ten movies in each group. The eclectic mix of\\ngenres within each movie group suggests that the rating behavior of users is\\nnot explained by genre alone.\\nFigure 2 presents a boxplot comparing the distributions of the movie\\nrelease years for each group. We can see a clear ordering of the movie groups\\nby median release date. It appears that the users in all three groups rate\\n9\\nmovies from all time periods, but reviewing behavior varies based on movie\\npopularity. The biclusters here suggest that individuals in group 3 are more\\nlikely to rate under-reviewed movies, whereas individuals in group 1 primarily\\nrate popular movies.\\nGroup 1 Group 2\\nMrs. Parker and the Vicious Circle (1994) Santa Clause, The (1994)\\nMiserables, Les (1995) Sleeper (1973)\\nLawnmower Man 2: Beyond Cyberspace (1996) Sword in the Stone, The (1963)\\nRichie Rich (1994) Cook the Thief His Wife & Her Lover, The (1989)\\nCandyman: Farewell to the Flesh (1995) Somewhere in Time (1980)\\nIce Storm, The (1997) Mulholland Falls (1996)\\nFunny Face (1957) Crumb (1994)\\nUmbrellas of Cherbourg, The (1964) I.Q. (1994)\\nMy Family (1995) Legends of the Fall (1994)\\nTop Hat (1935) Alice in Wonderland (1951)\\nGroup 3 Group 4\\nBeauty and the Beast (1991) Star Wars (1977)\\nBatman (1989) Contact (1997)\\nYoung Frankenstein (1974) Fargo (1996)\\nStar Trek IV: The Voyage Home (1986) Return of the Jedi (1983)\\nCitizen Kane (1941) Liar Liar (1997)\\nFifth Element, The (1997) English Patient, The (1996)\\nGandhi (1982) Scream (1996)\\nFace/Off (1997) Toy Story (1995)\\nDr. Strangelove Or (1963) Air Force One (1997)\\nTin Cup (1996) Independence Day (ID4) (1996)\\nTable 4: The top ten movies in each cluster based on the total number of\\nreviews.\\n10\\nlll\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nlll\\nl\\nl\\nl\\nll\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nll\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nll\\nl\\nll\\nll\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nlll\\nl\\nll\\nl\\nl\\nl\\nll\\nl\\nl\\nl\\nl\\nl\\nll\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nl\\nll\\nl\\nll\\nl\\nl\\nl\\nl\\nll\\nl\\nl\\nl\\nGroup 1 Group 2 Group 3 Group 4\\n19\\n20\\n19\\n40\\n19\\n60\\n19\\n80\\n20\\n00\\nRelease Years by Movie Group\\nM\\nov\\nie\\n Y\\ne\\na\\nr\\nFigure 6: Boxplot comparing the different clusters based on movie release\\ndates.\\n11\\n', 'id': 2448596, 'identifiers': [{'identifier': 'oai:arxiv.org:1206.6927', 'type': 'OAI_ID'}, {'identifier': '10.1214/19-ejs1667', 'type': 'DOI'}, {'identifier': '6206789', 'type': 'CORE_ID'}, {'identifier': '1206.6927', 'type': 'ARXIV_ID'}, {'identifier': '423162609', 'type': 'CORE_ID'}], 'title': 'Profile Likelihood Biclustering', 'language': {'code': 'en', 'name': 'English'}, 'magId': None, 'oaiIds': ['oai:arxiv.org:1206.6927'], 'publishedDate': '2020-06-02T00:00:00', 'publisher': \"'Institute of Mathematical Statistics'\", 'pubmedId': None, 'references': [], 'sourceFulltextUrls': ['http://arxiv.org/abs/1206.6927'], 'updatedDate': '2021-05-12T03:22:58', 'yearPublished': 2020, 'journals': [], 'links': [{'type': 'download', 'url': 'http://arxiv.org/abs/1206.6927'}, {'type': 'display', 'url': 'https://core.ac.uk/works/2448596'}]}\n"
     ]
    }
   ],
   "source": [
    "# Search query\n",
    "response = client.search(\n",
    "    index=index_name,\n",
    "    body={\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"size\": 1\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for hit in response['hits']['hits']:\n",
    "    print(hit['_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6deb4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
