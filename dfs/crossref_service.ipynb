{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee9f8a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "281c2d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrossrefService\").master(\"spark://spark-master:7077\").config(\"spark.cores.max\", \"2\").config(\"spark.executor.memory\", \"512m\").config(\"spark.eventLog.enabled\", \"true\").config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").getOrCreate()\n",
    "\n",
    "# Read JSON files from HDFS\n",
    "df = spark.read.json(\"all_papers.json\")\n",
    "\n",
    "def formatterParams(title, authors, id):\n",
    "    '''\n",
    "    Function to clean the title and authors name of the paper\n",
    "    \n",
    "    Parameters:\n",
    "    title (str): the title of the paper\n",
    "    authors (str): the name of the author\n",
    "    \n",
    "    Returns:\n",
    "    title (str): the cleaned title of the paper\n",
    "    authors (str): the cleaned name the name of the author\n",
    "    '''\n",
    "    if authors:\n",
    "        authors_str = authors[0]['name']\n",
    "    else:\n",
    "        authors_str = ''\n",
    "    return title, authors_str, id\n",
    "\n",
    "def find_publisher_location(json_data):\n",
    "    '''\n",
    "    Function to find the publisher location in the JSON response\n",
    "\n",
    "    Parameters:\n",
    "    json_data (dict): the JSON response from the API\n",
    "\n",
    "    Returns:\n",
    "    str: the publisher location if found, otherwise None\n",
    "    '''\n",
    "    if isinstance(json_data, dict):\n",
    "        for key, value in json_data.items():\n",
    "            if key == \"publisher-location\":\n",
    "                return value\n",
    "            elif isinstance(value, (dict, list)):\n",
    "                result = find_publisher_location(value)\n",
    "                if result:\n",
    "                    return result\n",
    "    elif isinstance(json_data, list):\n",
    "        for item in json_data:\n",
    "            result = find_publisher_location(item)\n",
    "            if result:\n",
    "                return result\n",
    "    return None\n",
    "\n",
    "def process_paper(paper):\n",
    "    '''\n",
    "    Function to process each paper asynchronously\n",
    "\n",
    "    Parameters:\n",
    "    paper (dict): paper information from JSON data\n",
    "    '''\n",
    "    title = paper['title']\n",
    "    authors = paper['authors']\n",
    "    id = paper['id']\n",
    "    title, authors, id = formatterParams(title, authors, id)\n",
    "    url = f\"https://api.crossref.org/works?query.author={authors}&query.title={title}\"\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        response = session.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            publisher_location = find_publisher_location(data)\n",
    "            if publisher_location:\n",
    "                # Añadir los datos del título del artículo y la ubicación del editor a la lista\n",
    "                return {'id': id, 'publisher_location': publisher_location}\n",
    "            else:\n",
    "                return {'id': id, 'publisher_location': 'No location found'}\n",
    "        else:\n",
    "            return {'id': id, 'publisher_location': 'API call failed'}\n",
    "\n",
    "# Convert DataFrame to RDD and apply process_paper function\n",
    "rdd = df.rdd.map(lambda row: process_paper(row.asDict()))\n",
    "\n",
    "# Convert RDD back to DataFrame\n",
    "result_df = rdd.toDF([\"id\", \"publisher_location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7098b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 2) / 2]"
     ]
    }
   ],
   "source": [
    "# Write the result to a CSV file\n",
    "result_df.write.csv(\"dataout/paper_location\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed109fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]"
     ]
    }
   ],
   "source": [
    "# Write the result to a CSV file\n",
    "#result_df.coalesce(1).write.csv(\"paper_location\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f51819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294ac00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
