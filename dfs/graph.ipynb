{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49a513d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T09:57:14.744793Z",
     "iopub.status.busy": "2024-06-22T09:57:14.744436Z",
     "iopub.status.idle": "2024-06-22T09:57:20.788576Z",
     "shell.execute_reply": "2024-06-22T09:57:20.786359Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/22 09:57:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, lit\n",
    "import json\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Graph\").master(\"spark://spark-master:7077\").config(\"spark.executor.memory\", \"512m\").config(\"spark.eventLog.enabled\", \"true\").config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "447ab28d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T09:57:20.797924Z",
     "iopub.status.busy": "2024-06-22T09:57:20.796756Z",
     "iopub.status.idle": "2024-06-22T09:57:32.596295Z",
     "shell.execute_reply": "2024-06-22T09:57:32.595265Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load paper_dates data\n",
    "paper_dates_df = spark.read.csv(\"file:///opt/workspace/tabular/paper_dates/*.csv\", header=True)\n",
    "paper_dates_df = paper_dates_df.withColumnRenamed(\"month\", \"publish_month\")\n",
    "\n",
    "# Load paper_location data\n",
    "#paper_location_df = spark.read.csv(\"file:///opt/workspace/tabular/paper_location/*.csv\", header=True)\n",
    "\n",
    "# Load paper_theme data\n",
    "paper_theme_df = spark.read.csv(\"file:///opt/workspace/tabular/paper_theme/*.csv\", header=True)\n",
    "\n",
    "# Load paper_publishers data\n",
    "paper_publishers_df = spark.read.csv(\"file:///opt/workspace/tabular/paper_publishers/*.csv\", header=True)\n",
    "\n",
    "# Load all_papers.json data\n",
    "authors_df = spark.read.json(\"all_papers.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6b52236",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T09:57:32.601345Z",
     "iopub.status.busy": "2024-06-22T09:57:32.600741Z",
     "iopub.status.idle": "2024-06-22T09:57:46.489766Z",
     "shell.execute_reply": "2024-06-22T09:57:46.487749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Join paper_dates, paper_theme, and paper_publishers on 'id'\n",
    "papers_df = paper_dates_df.join(paper_theme_df, \"id\", \"left\") \\\n",
    "    .join(paper_publishers_df, \"id\", \"left\")\n",
    "\n",
    "# Explode the authors array to create multiple rows for each author\n",
    "authors_df = authors_df.withColumn(\"author\", explode(col(\"authors\"))) \\\n",
    "                       .select(\"id\", col(\"author.name\").alias(\"name\"))\n",
    "\n",
    "# Create CSV files for relationships\n",
    "# Relationships between paper and author (posted_paper)\n",
    "\n",
    "# Aggregate papers_df to select only one row per \"id\"\n",
    "papers_agg_df = papers_df.groupby(\"id\").agg(first(\"publish_month\").alias(\"publish_month\"), first(\"day_of_week\").alias(\"day_of_week\"))\n",
    "\n",
    "# Join the aggregated papers_df with authors_df\n",
    "papers_authors_relationships = papers_agg_df.join(authors_df, \"id\", \"inner\") \\\n",
    "    .select(\"id\", col(\"name\").alias(\"author\"), \"publish_month\", \"day_of_week\")\n",
    "\n",
    "papers_authors_relationships.coalesce(1).write.csv(\"dataout/posted_paper\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Relationships between author and publisher (posted_with)\n",
    "authors_publishers_relationships = authors_df.join(paper_publishers_df, \"id\", \"inner\") \\\n",
    "    .select(\"name\", \"publisher\", \"description\") \\\n",
    "    .withColumnRenamed(\"name\", \"author\")\n",
    "\n",
    "authors_publishers_relationships.coalesce(1).write.csv(\"dataout/posted_with\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Relationships between author and author (published_with)\n",
    "published_with_relationships = authors_df.alias(\"a1\").join(authors_df.alias(\"a2\"), \"id\", \"inner\") \\\n",
    "    .filter(col(\"a1.name\") < col(\"a2.name\")) \\\n",
    "    .select(col(\"a1.name\").alias(\"author_1\"), col(\"a2.name\").alias(\"author_2\"))\n",
    "\n",
    "published_with_relationships.coalesce(1).write.csv(\"dataout/published_with\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Create CSV files for nodes\n",
    "papers_df.select(\"id\",\"theme\").coalesce(1).write.csv(\"dataout/papers\", header=True, mode=\"overwrite\")\n",
    "\n",
    "authors_df.select(\"name\").distinct().withColumnRenamed(\"name\", \"author\").coalesce(1).write.csv(\"dataout/authors\", header=True, mode=\"overwrite\")\n",
    "\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "paper_publishers_df.select(\"publisher\", \"description\").coalesce(1).write.csv(\"dataout/publishers\", header=True, mode=\"overwrite\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e57fb85c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T09:57:46.495765Z",
     "iopub.status.busy": "2024-06-22T09:57:46.495285Z",
     "iopub.status.idle": "2024-06-22T09:57:47.018146Z",
     "shell.execute_reply": "2024-06-22T09:57:47.016722Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
