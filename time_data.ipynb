{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_fi10eU0J97"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import requests\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Time_services\").master(\"spark://spark-master_new:7077\").config(\"spark.executor.memory\", \"512m\").config(\"spark.eventLog.enabled\", \"true\").config(\"spark.eventLog.dir\", \"file:///opt/workspace/events\").getOrCreate()\n",
        "\n",
        "# Read JSON files from HDFS\n",
        "df = spark.read.json(\"all_papers.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transform_row(row):\n",
        "    json_id = row['id']\n",
        "    text = row['title']  # Choose 'abstract' or 'title'\n",
        "\n",
        "    result = call_hugging_face_api(text)\n",
        "    date_string = result['date'] \n",
        "\n",
        "    #cambiar formato\n",
        "    date_object = datetime.strptime(date_string, \"%Y-%m-%dT%H:%M:%S\")\n",
        "\n",
        "    formatted_date = {\n",
        "        \"day\": date_object.day,\n",
        "        \"month\": date_object.month,\n",
        "        \"year\": date_object.year,\n",
        "        \"weekday\": date_object.strftime(\"%A\")\n",
        "    }\n",
        "\n",
        "    formatted_date_json = json.dumps(formatted_date)\n",
        "\n",
        "    return (json_id, formatted_date_json)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transformed_data = df.rdd.map(transform_row)\n",
        "\n",
        "transformed_df = transformed_data.toDF([\"id\", \"formatted_date\"])    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Write the result to a CSV file\n",
        "result_df.write.csv(\"output/papers_with_locations.csv\", header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Te damos la bienvenida a Colaboratory",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
