When comparing Hadoop MapReduce with Apache Spark for tasks like enriching .json files containing paper data with sentiment analysis scores, Spark generally offers several advantages over Hadoop MapReduce in terms of ease of use, performance, and flexibility. Here's why:

### Apache Spark:

1. **Ease of Use**: Spark provides higher-level APIs in languages like Scala, Java, Python, and R, which are more developer-friendly compared to Hadoop MapReduce. Spark's DataFrame and Dataset APIs offer simpler and more expressive ways to manipulate data compared to raw MapReduce programming.

2. **Performance**: Spark is known for its in-memory processing capabilities, which can result in significantly faster processing times compared to Hadoop MapReduce, especially for iterative algorithms and interactive data analysis. Spark's ability to cache data in memory between multiple operations reduces the need for repeated disk I/O, leading to faster execution.

3. **Flexibility**: Spark offers a wide range of libraries and built-in functions for various data processing tasks, including SQL queries, machine learning, graph processing, and streaming. This flexibility allows you to perform multiple types of data enrichment and analysis within the same framework, without having to switch between different tools or systems.

4. **Unified Processing Engine**: Spark provides a unified processing engine that supports batch processing, interactive queries, stream processing, and machine learning within the same runtime environment. This simplifies the development and deployment of complex data pipelines and analytical applications.

5. **Community and Ecosystem**: Spark has a large and active open-source community, as well as a rich ecosystem of third-party libraries and tools. This vibrant ecosystem provides access to a wide range of resources, documentation, tutorials, and support forums, making it easier to learn and use Spark effectively.

### Conclusion:

While it's possible to accomplish tasks like enriching .json files with sentiment analysis scores using Hadoop MapReduce, Apache Spark generally offers a more efficient, developer-friendly, and feature-rich platform for such tasks. Spark's higher-level abstractions, in-memory processing, and comprehensive ecosystem make it a preferred choice for many data processing and analytics use cases, including data enrichment and transformation. Therefore, for tasks like the one you described, using Apache Spark is likely to be easier and more productive compared to Hadoop MapReduce.

Como están las carpetas antes de ejecutar:
- coordinator: coordinator.py, Dockerfile
- data: info.md 
- dfs: info.md, text_classification.ipynb
- docker-configuration: .env, docker_compose_configuration, docker-compose.yml.template, Dockerfile, env_vars
- merger: Dockerfile, merge.py
- spark: docker-compose, Dockerfile.worker
- worker: Dockerfile, worker.py, requirements
 
Afuera: LICENSE, notes, README, Repository
--------------------------------------------------------------------
en bash:

'''
you may need

cd PapersLab
cd docker-configuration
$PWD_PARENT = Split-Path -Parent $PWD
$env:PWD_PARENT = $PWD_PARENT

then you continue on step 2
'''
1. export PWD_PARENT=$(dirname "$(pwd)")

2. docker build -t docker_configuration .

3. docker run -d --name docker_configuration -v PWD_PARENT:/app/project docker_configuration

4. docker ps -a --filter "name=docker_configuration" --format "{{.ID}}"

5. Utilice el output ID delpaso anterior : docker cp ID:/app/docker-compose.yml $(dirname "$(pwd)")

	example: docker cp c2543cf66461:/app/docker-compose.yml $(dirname "$(pwd)")

6. cd ..

7. docker-compose up --build --detach

-------------------------------------------------------------------------------------

Una vez tenemos el archivo all_data.json en dfs folder

1. cd ./spark
2. ./build.sh
2. docker-compose --env-file .env up --build --detach

Para desmontar todo:
en /spark:  docker compose down
cd ../docker-configuration
docker compose down

y en docker borrar de:
- contenedores docker_configuration
- images, build y volumes lo que sea dependiente de este proyecto

Así está todo limpito en el ordenador :D
------------------------------------------------------------------------------------------

bases de datos 
cd ..
cd databases
docker compose up (neo4j buil in localhost:7474)

----------------------------------------------------------------
1. **Importing nodes:**
```cypher
LOAD CSV WITH HEADERS FROM 'file:///papers.csv' AS row
CREATE (:Paper {id: row.id, theme: row.theme});
```

```cypher
LOAD CSV WITH HEADERS FROM 'file:///authors.csv' AS row
MERGE (:Author {name: row.author});
```

```cypher
LOAD CSV WITH HEADERS FROM 'file:///publishers.csv' AS row
CREATE (:Publisher {publisher: row.publisher, description: row.description});
```

2. **Importing relationships:**
```cypher
LOAD CSV WITH HEADERS FROM 'file:///posted_paper.csv' AS row
MATCH (p:Paper {id: row.id})
MATCH (a:Author {name: row.author})
MERGE (a)-[:POSTED]->(p);
```

```cypher
LOAD CSV WITH HEADERS FROM 'file:///posted_with.csv' AS row
MATCH (a:Author {name: row.author})
MATCH (pub:Publisher {publisher: row.publisher, description: row.description})
MERGE (a)-[:POSTED_WITH]->(pub);
```

```cypher
LOAD CSV WITH HEADERS FROM 'file:///published_with.csv' AS row
MATCH (a1:Author {name: row.author_1})
MATCH (a2:Author {name: row.author_2})
MERGE (a1)-[:PUBLISHED_WITH]->(a2);
```

Make sure to replace `'file:///papers.csv'`, `'file:///authors.csv'`, etc., with the correct file paths where your CSV files are located. Also, ensure that the column names in the CSV files match the property names used in the Cypher queries.

---------------------------
Elasticsearch
Pros:

Full-Text Search: Elasticsearch is designed for full-text search and provides powerful search capabilities, such as phrase matching, relevance scoring, and complex querying.
Scalability: It scales horizontally and can handle large volumes of data and high query loads.
Analytics: Supports real-time analytics and aggregations.
Use Cases:

Searching and analyzing large volumes of text data (e.g., logs, articles, product descriptions).
Real-time search applications.